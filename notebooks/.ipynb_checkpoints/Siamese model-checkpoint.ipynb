{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44776ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3ff2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\t• TensorFlow version: 1.15.5\n",
      "\t• tf.keras version: 2.2.4-tf\n",
      "\t• Running on GPU\n"
     ]
    }
   ],
   "source": [
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "print('Using:')\n",
    "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
    "print('\\t\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d443a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images belonging to 3 classes.\n",
      "The train set contains 30\n",
      "Found 30 images belonging to 3 classes.\n",
      "The valid set contains 30\n",
      "Found 648 images belonging to 3 classes.\n",
      "The test set contains 648\n"
     ]
    }
   ],
   "source": [
    "basedir = os.path.join(\"C:\\\\Users\\\\manos\\\\git\\\\metacovid-siamese-neural-network\", \"dataset\", \"siamese\") \n",
    "\n",
    "train_image_list, train_y_list = utils.load_images(basedir, 'train', (100,100))\n",
    "print(\"The train set contains\",len(train_image_list)) \n",
    "\n",
    "valid_image_list, valid_y_list = utils.load_images(basedir, 'validation', (100,100))   \n",
    "print(\"The valid set contains\", len(valid_image_list))  \n",
    "\n",
    "test_image_list, test_y_list = utils.load_images(basedir, 'test', (100,100))   \n",
    "print(\"The test set contains\", len(test_image_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59148d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairs for training 60\n",
      "number of pairs for validation 60\n",
      "number of pairs for test 1296\n"
     ]
    }
   ],
   "source": [
    "# make train pairs\n",
    "pairs_train, labels_train = utils.make_pairs(train_image_list, train_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_val, labels_val = utils.make_pairs(valid_image_list, valid_y_list)\n",
    "\n",
    "# make test pairs\n",
    "pairs_test, labels_test = utils.make_pairs(test_image_list, test_y_list)\n",
    "\n",
    "x_train_1 = pairs_train[:, 0]  \n",
    "x_train_2 = pairs_train[:, 1]\n",
    "print(\"number of pairs for training\", np.shape(x_train_1)[0]) \n",
    "\n",
    "x_val_1 = pairs_val[:, 0] \n",
    "x_val_2 = pairs_val[:, 1]\n",
    "print(\"number of pairs for validation\", np.shape(x_val_1)[0]) \n",
    "\n",
    "x_test_1 = pairs_test[:, 0] \n",
    "x_test_2 = pairs_test[:, 1]\n",
    "print(\"number of pairs for test\", np.shape(x_test_1)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509c04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 5120)         14748995    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 14,748,997\n",
      "Trainable params: 20,482\n",
      "Non-trainable params: 14,728,515\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "SIAMESE_MODEL_FNAME = 'siamese_network.h5'\n",
    "EMBEDDING_MODEL_FNAME = 'embedding_network.h5'\n",
    "\n",
    "input_1 = Input((100,100,3))\n",
    "input_2 = Input((100,100,3))\n",
    "\n",
    "embedding_network = tf.keras.models.load_model(EMBEDDING_MODEL_FNAME)\n",
    "embedding_network.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential() \n",
    "for layer in embedding_network.layers:  \n",
    "    model.add(layer) \n",
    "\n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dense(5120, name='den', activation='sigmoid', kernel_regularizer='l2')) \n",
    " \n",
    "output_1 = model(input_1) \n",
    "output_2 = model(input_2) \n",
    " \n",
    "merge_layer = Lambda(utils.manhattan_distance)([output_1, output_2]) \n",
    "output_layer = Dense(1, activation=\"sigmoid\")(merge_layer) \n",
    "siamese = Model(inputs=[input_1, input_2], outputs=output_layer) \n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d473e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" callbacks \"\"\"\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.0001)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='siamese_network.h5', verbose=1, \n",
    "                                save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d74c7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 5120)         14748995    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 14,748,997\n",
      "Trainable params: 20,482\n",
      "Non-trainable params: 14,728,515\n",
      "__________________________________________________________________________________________________\n",
      "Train on 60 samples, validate on 60 samples\n",
      "Epoch 1/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.5254\n",
      "Epoch 00001: val_loss improved from inf to 0.25762, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 139ms/sample - loss: 0.1930 - acc: 0.5333 - val_loss: 0.2576 - val_acc: 0.5667\n",
      "Epoch 2/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.5763\n",
      "Epoch 00002: val_loss improved from 0.25762 to 0.22631, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.1702 - acc: 0.5833 - val_loss: 0.2263 - val_acc: 0.6167\n",
      "Epoch 3/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.6610\n",
      "Epoch 00003: val_loss improved from 0.22631 to 0.19832, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.1537 - acc: 0.6667 - val_loss: 0.1983 - val_acc: 0.6333\n",
      "Epoch 4/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.7288\n",
      "Epoch 00004: val_loss improved from 0.19832 to 0.17962, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 126ms/sample - loss: 0.1427 - acc: 0.7333 - val_loss: 0.1796 - val_acc: 0.6500\n",
      "Epoch 5/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.7797\n",
      "Epoch 00005: val_loss improved from 0.17962 to 0.17215, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 127ms/sample - loss: 0.1375 - acc: 0.7833 - val_loss: 0.1722 - val_acc: 0.6667\n",
      "Epoch 6/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.8136\n",
      "Epoch 00006: val_loss improved from 0.17215 to 0.17130, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.1350 - acc: 0.8000 - val_loss: 0.1713 - val_acc: 0.6833\n",
      "Epoch 7/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.8305- ETA: 0s - loss: 0.1285 - acc:\n",
      "Epoch 00007: val_loss improved from 0.17130 to 0.16843, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.1334 - acc: 0.8167 - val_loss: 0.1684 - val_acc: 0.7000\n",
      "Epoch 8/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.8136- ETA: 5s - loss: 0 \n",
      "Epoch 00008: val_loss improved from 0.16843 to 0.16766, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1322 - acc: 0.8167 - val_loss: 0.1677 - val_acc: 0.7000\n",
      "Epoch 9/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.8136\n",
      "Epoch 00009: val_loss improved from 0.16766 to 0.16683, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1312 - acc: 0.8167 - val_loss: 0.1668 - val_acc: 0.7000\n",
      "Epoch 10/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.8475\n",
      "Epoch 00010: val_loss improved from 0.16683 to 0.16590, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1303 - acc: 0.8333 - val_loss: 0.1659 - val_acc: 0.7167\n",
      "Epoch 11/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.8475\n",
      "Epoch 00011: val_loss improved from 0.16590 to 0.16509, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1297 - acc: 0.8500 - val_loss: 0.1651 - val_acc: 0.7167\n",
      "Epoch 12/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.8475\n",
      "Epoch 00012: val_loss improved from 0.16509 to 0.16436, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1292 - acc: 0.8500 - val_loss: 0.1644 - val_acc: 0.7167\n",
      "Epoch 13/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.8983\n",
      "Epoch 00013: val_loss improved from 0.16436 to 0.16394, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1285 - acc: 0.8833 - val_loss: 0.1639 - val_acc: 0.7167\n",
      "Epoch 14/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9153\n",
      "Epoch 00014: val_loss did not improve from 0.16394\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.1281 - acc: 0.9000 - val_loss: 0.1653 - val_acc: 0.7167\n",
      "Epoch 15/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.8983\n",
      "Epoch 00015: val_loss improved from 0.16394 to 0.16221, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.1275 - acc: 0.9000 - val_loss: 0.1622 - val_acc: 0.7167\n",
      "Epoch 16/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.8983\n",
      "Epoch 00016: val_loss did not improve from 0.16221\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.1271 - acc: 0.9000 - val_loss: 0.1628 - val_acc: 0.7167\n",
      "Epoch 17/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.8983\n",
      "Epoch 00017: val_loss improved from 0.16221 to 0.16076, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.1264 - acc: 0.9000 - val_loss: 0.1608 - val_acc: 0.7333\n",
      "Epoch 18/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.8983\n",
      "Epoch 00018: val_loss did not improve from 0.16076\n",
      "60/60 [==============================] - 8s 133ms/sample - loss: 0.1261 - acc: 0.9000 - val_loss: 0.1610 - val_acc: 0.7333\n",
      "Epoch 19/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9153\n",
      "Epoch 00019: val_loss did not improve from 0.16076\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.1254 - acc: 0.9000 - val_loss: 0.1620 - val_acc: 0.7333\n",
      "Epoch 20/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.8983\n",
      "Epoch 00020: val_loss did not improve from 0.16076\n",
      "60/60 [==============================] - 8s 132ms/sample - loss: 0.1251 - acc: 0.9000 - val_loss: 0.1620 - val_acc: 0.7333\n",
      "Epoch 21/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.8983\n",
      "Epoch 00021: val_loss improved from 0.16076 to 0.15933, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.1245 - acc: 0.9000 - val_loss: 0.1593 - val_acc: 0.7500\n",
      "Epoch 22/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.8983\n",
      "Epoch 00022: val_loss improved from 0.15933 to 0.15852, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1241 - acc: 0.9000 - val_loss: 0.1585 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.8983\n",
      "Epoch 00023: val_loss did not improve from 0.15852\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1235 - acc: 0.9000 - val_loss: 0.1599 - val_acc: 0.7500\n",
      "Epoch 24/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.8983\n",
      "Epoch 00024: val_loss did not improve from 0.15852\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.1233 - acc: 0.9000 - val_loss: 0.1613 - val_acc: 0.7500\n",
      "Epoch 25/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.8983\n",
      "Epoch 00025: val_loss did not improve from 0.15852\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1226 - acc: 0.9000 - val_loss: 0.1590 - val_acc: 0.7667\n",
      "Epoch 26/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.8983\n",
      "Epoch 00026: val_loss improved from 0.15852 to 0.15775, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1220 - acc: 0.9000 - val_loss: 0.1577 - val_acc: 0.7833\n",
      "Epoch 27/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.8983\n",
      "Epoch 00027: val_loss improved from 0.15775 to 0.15653, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.1214 - acc: 0.9000 - val_loss: 0.1565 - val_acc: 0.8000\n",
      "Epoch 28/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9153\n",
      "Epoch 00028: val_loss did not improve from 0.15653\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1211 - acc: 0.9167 - val_loss: 0.1576 - val_acc: 0.8000\n",
      "Epoch 29/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9153\n",
      "Epoch 00029: val_loss did not improve from 0.15653\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.1205 - acc: 0.9167 - val_loss: 0.1568 - val_acc: 0.8000\n",
      "Epoch 30/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9153\n",
      "Epoch 00030: val_loss did not improve from 0.15653\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1201 - acc: 0.9167 - val_loss: 0.1567 - val_acc: 0.8000\n",
      "Epoch 31/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9322\n",
      "Epoch 00031: val_loss improved from 0.15653 to 0.15599, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1197 - acc: 0.9333 - val_loss: 0.1560 - val_acc: 0.8000\n",
      "Epoch 32/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9492\n",
      "Epoch 00032: val_loss improved from 0.15599 to 0.15520, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 138ms/sample - loss: 0.1191 - acc: 0.9500 - val_loss: 0.1552 - val_acc: 0.8000\n",
      "Epoch 33/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9492\n",
      "Epoch 00033: val_loss improved from 0.15520 to 0.15415, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.1186 - acc: 0.9500 - val_loss: 0.1542 - val_acc: 0.8000\n",
      "Epoch 34/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9831\n",
      "Epoch 00034: val_loss did not improve from 0.15415\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1181 - acc: 0.9833 - val_loss: 0.1546 - val_acc: 0.8000\n",
      "Epoch 35/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9492- ETA: 1s - loss:\n",
      "Epoch 00035: val_loss improved from 0.15415 to 0.15364, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.1177 - acc: 0.9500 - val_loss: 0.1536 - val_acc: 0.8000\n",
      "Epoch 36/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9831\n",
      "Epoch 00036: val_loss did not improve from 0.15364\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.1173 - acc: 0.9833 - val_loss: 0.1554 - val_acc: 0.8000\n",
      "Epoch 37/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9492\n",
      "Epoch 00037: val_loss improved from 0.15364 to 0.15218, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.1167 - acc: 0.9500 - val_loss: 0.1522 - val_acc: 0.8000\n",
      "Epoch 38/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9831- ETA: 1s - \n",
      "Epoch 00038: val_loss did not improve from 0.15218\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1163 - acc: 0.9833 - val_loss: 0.1535 - val_acc: 0.8000\n",
      "Epoch 39/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9831\n",
      "Epoch 00039: val_loss did not improve from 0.15218\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.1159 - acc: 0.9833 - val_loss: 0.1531 - val_acc: 0.8000\n",
      "Epoch 40/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9831\n",
      "Epoch 00040: val_loss did not improve from 0.15218\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.1155 - acc: 0.9833 - val_loss: 0.1523 - val_acc: 0.8000\n",
      "Epoch 41/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9831\n",
      "Epoch 00041: val_loss improved from 0.15218 to 0.15211, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.1149 - acc: 0.9833 - val_loss: 0.1521 - val_acc: 0.8000\n",
      "Epoch 42/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9831\n",
      "Epoch 00042: val_loss improved from 0.15211 to 0.15138, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.1144 - acc: 0.9833 - val_loss: 0.1514 - val_acc: 0.8000\n",
      "Epoch 43/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9831- ETA: 0s - loss: 0.1180 - acc: 0.9\n",
      "Epoch 00043: val_loss improved from 0.15138 to 0.15108, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1139 - acc: 0.9833 - val_loss: 0.1511 - val_acc: 0.8000\n",
      "Epoch 44/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9831\n",
      "Epoch 00044: val_loss improved from 0.15108 to 0.14929, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.1136 - acc: 0.9833 - val_loss: 0.1493 - val_acc: 0.8167\n",
      "Epoch 45/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9831\n",
      "Epoch 00045: val_loss improved from 0.14929 to 0.14884, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.1131 - acc: 0.9833 - val_loss: 0.1488 - val_acc: 0.8333\n",
      "Epoch 46/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9831\n",
      "Epoch 00046: val_loss did not improve from 0.14884\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.1127 - acc: 0.9833 - val_loss: 0.1512 - val_acc: 0.8167\n",
      "Epoch 47/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9831\n",
      "Epoch 00047: val_loss improved from 0.14884 to 0.14853, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1121 - acc: 0.9833 - val_loss: 0.1485 - val_acc: 0.8500\n",
      "Epoch 48/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9831\n",
      "Epoch 00048: val_loss did not improve from 0.14853\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.1116 - acc: 0.9833 - val_loss: 0.1489 - val_acc: 0.8333\n",
      "Epoch 49/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9831\n",
      "Epoch 00049: val_loss improved from 0.14853 to 0.14810, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.1113 - acc: 0.9833 - val_loss: 0.1481 - val_acc: 0.8500\n",
      "Epoch 50/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9831\n",
      "Epoch 00050: val_loss improved from 0.14810 to 0.14741, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1107 - acc: 0.9833 - val_loss: 0.1474 - val_acc: 0.8500\n",
      "Epoch 51/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9831\n",
      "Epoch 00051: val_loss improved from 0.14741 to 0.14692, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.1103 - acc: 0.9833 - val_loss: 0.1469 - val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9831\n",
      "Epoch 00052: val_loss did not improve from 0.14692\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.1099 - acc: 0.9833 - val_loss: 0.1472 - val_acc: 0.8500\n",
      "Epoch 53/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9831\n",
      "Epoch 00053: val_loss improved from 0.14692 to 0.14591, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1093 - acc: 0.9833 - val_loss: 0.1459 - val_acc: 0.8667\n",
      "Epoch 54/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9831\n",
      "Epoch 00054: val_loss improved from 0.14591 to 0.14532, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.1089 - acc: 0.9833 - val_loss: 0.1453 - val_acc: 0.8667\n",
      "Epoch 55/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9831\n",
      "Epoch 00055: val_loss did not improve from 0.14532\n",
      "60/60 [==============================] - 7s 114ms/sample - loss: 0.1086 - acc: 0.9833 - val_loss: 0.1459 - val_acc: 0.8667\n",
      "Epoch 56/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9831\n",
      "Epoch 00056: val_loss improved from 0.14532 to 0.14475, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1080 - acc: 0.9833 - val_loss: 0.1448 - val_acc: 0.8667\n",
      "Epoch 57/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9831\n",
      "Epoch 00057: val_loss did not improve from 0.14475\n",
      "60/60 [==============================] - 8s 131ms/sample - loss: 0.1075 - acc: 0.9833 - val_loss: 0.1454 - val_acc: 0.8667\n",
      "Epoch 58/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9831\n",
      "Epoch 00058: val_loss improved from 0.14475 to 0.14362, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 126ms/sample - loss: 0.1071 - acc: 0.9833 - val_loss: 0.1436 - val_acc: 0.8667\n",
      "Epoch 59/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9831\n",
      "Epoch 00059: val_loss did not improve from 0.14362\n",
      "60/60 [==============================] - 8s 129ms/sample - loss: 0.1067 - acc: 0.9833 - val_loss: 0.1443 - val_acc: 0.8667\n",
      "Epoch 60/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9831\n",
      "Epoch 00060: val_loss did not improve from 0.14362\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.1062 - acc: 0.9833 - val_loss: 0.1440 - val_acc: 0.8667\n",
      "Epoch 61/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9831\n",
      "Epoch 00061: val_loss did not improve from 0.14362\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1059 - acc: 0.9833 - val_loss: 0.1459 - val_acc: 0.8667\n",
      "Epoch 62/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9831\n",
      "Epoch 00062: val_loss improved from 0.14362 to 0.14152, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.1055 - acc: 0.9833 - val_loss: 0.1415 - val_acc: 0.8667\n",
      "Epoch 63/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9831\n",
      "Epoch 00063: val_loss did not improve from 0.14152\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.1050 - acc: 0.9833 - val_loss: 0.1442 - val_acc: 0.8667\n",
      "Epoch 64/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9831\n",
      "Epoch 00064: val_loss improved from 0.14152 to 0.14141, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1046 - acc: 0.9833 - val_loss: 0.1414 - val_acc: 0.8667\n",
      "Epoch 65/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9831\n",
      "Epoch 00065: val_loss did not improve from 0.14141\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.1040 - acc: 0.9833 - val_loss: 0.1423 - val_acc: 0.8667\n",
      "Epoch 66/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9831\n",
      "Epoch 00066: val_loss improved from 0.14141 to 0.14074, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.1036 - acc: 0.9833 - val_loss: 0.1407 - val_acc: 0.8667\n",
      "Epoch 67/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9831\n",
      "Epoch 00067: val_loss improved from 0.14074 to 0.14039, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 125ms/sample - loss: 0.1032 - acc: 0.9833 - val_loss: 0.1404 - val_acc: 0.8667\n",
      "Epoch 68/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9831\n",
      "Epoch 00068: val_loss improved from 0.14039 to 0.13960, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.1027 - acc: 0.9833 - val_loss: 0.1396 - val_acc: 0.8667\n",
      "Epoch 69/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 1.0000\n",
      "Epoch 00069: val_loss did not improve from 0.13960\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.1025 - acc: 0.9833 - val_loss: 0.1409 - val_acc: 0.8667\n",
      "Epoch 70/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9831- ETA: 0s - loss: 0.0987 - ac\n",
      "Epoch 00070: val_loss improved from 0.13960 to 0.13882, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.1019 - acc: 0.9833 - val_loss: 0.1388 - val_acc: 0.8667\n",
      "Epoch 71/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9831\n",
      "Epoch 00071: val_loss did not improve from 0.13882\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.1015 - acc: 0.9833 - val_loss: 0.1394 - val_acc: 0.8667\n",
      "Epoch 72/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9831\n",
      "Epoch 00072: val_loss improved from 0.13882 to 0.13874, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.1010 - acc: 0.9833 - val_loss: 0.1387 - val_acc: 0.8667\n",
      "Epoch 73/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9831\n",
      "Epoch 00073: val_loss improved from 0.13874 to 0.13796, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1007 - acc: 0.9833 - val_loss: 0.1380 - val_acc: 0.8667\n",
      "Epoch 74/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9831\n",
      "Epoch 00074: val_loss did not improve from 0.13796\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.1004 - acc: 0.9833 - val_loss: 0.1411 - val_acc: 0.8667\n",
      "Epoch 75/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9831\n",
      "Epoch 00075: val_loss improved from 0.13796 to 0.13732, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0998 - acc: 0.9833 - val_loss: 0.1373 - val_acc: 0.8667\n",
      "Epoch 76/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9831\n",
      "Epoch 00076: val_loss did not improve from 0.13732\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0994 - acc: 0.9833 - val_loss: 0.1383 - val_acc: 0.8667\n",
      "Epoch 77/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9831\n",
      "Epoch 00077: val_loss did not improve from 0.13732\n",
      "60/60 [==============================] - 7s 114ms/sample - loss: 0.0990 - acc: 0.9833 - val_loss: 0.1382 - val_acc: 0.8667\n",
      "Epoch 78/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9831\n",
      "Epoch 00078: val_loss improved from 0.13732 to 0.13421, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.0985 - acc: 0.9833 - val_loss: 0.1342 - val_acc: 0.8667\n",
      "Epoch 79/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9831\n",
      "Epoch 00079: val_loss did not improve from 0.13421\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0981 - acc: 0.9833 - val_loss: 0.1352 - val_acc: 0.8667\n",
      "Epoch 80/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9831\n",
      "Epoch 00080: val_loss did not improve from 0.13421\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.0977 - acc: 0.9833 - val_loss: 0.1379 - val_acc: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9831\n",
      "Epoch 00081: val_loss did not improve from 0.13421\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0972 - acc: 0.9833 - val_loss: 0.1348 - val_acc: 0.8667\n",
      "Epoch 82/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 1.0000\n",
      "Epoch 00082: val_loss did not improve from 0.13421\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.0970 - acc: 1.0000 - val_loss: 0.1368 - val_acc: 0.8667\n",
      "Epoch 83/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9831- ETA: 0s - loss: 0.1014 - acc: \n",
      "Epoch 00083: val_loss improved from 0.13421 to 0.13393, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.0964 - acc: 0.9833 - val_loss: 0.1339 - val_acc: 0.8667\n",
      "Epoch 84/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9831\n",
      "Epoch 00084: val_loss did not improve from 0.13393\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0962 - acc: 0.9833 - val_loss: 0.1342 - val_acc: 0.8667\n",
      "Epoch 85/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9831\n",
      "Epoch 00085: val_loss did not improve from 0.13393\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0956 - acc: 0.9833 - val_loss: 0.1339 - val_acc: 0.8667\n",
      "Epoch 86/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9831\n",
      "Epoch 00086: val_loss improved from 0.13393 to 0.13289, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0952 - acc: 0.9833 - val_loss: 0.1329 - val_acc: 0.8667\n",
      "Epoch 87/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 1.0000\n",
      "Epoch 00087: val_loss improved from 0.13289 to 0.13275, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.0947 - acc: 1.0000 - val_loss: 0.1327 - val_acc: 0.8667\n",
      "Epoch 88/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9831- ETA: 1s - loss: 0.087\n",
      "Epoch 00088: val_loss improved from 0.13275 to 0.13056, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.0944 - acc: 0.9833 - val_loss: 0.1306 - val_acc: 0.8667\n",
      "Epoch 89/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 1.0000\n",
      "Epoch 00089: val_loss did not improve from 0.13056\n",
      "60/60 [==============================] - 7s 114ms/sample - loss: 0.0940 - acc: 1.0000 - val_loss: 0.1331 - val_acc: 0.8667\n",
      "Epoch 90/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9831\n",
      "Epoch 00090: val_loss did not improve from 0.13056\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0935 - acc: 0.9833 - val_loss: 0.1318 - val_acc: 0.8667\n",
      "Epoch 91/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 1.0000\n",
      "Epoch 00091: val_loss did not improve from 0.13056\n",
      "60/60 [==============================] - 7s 114ms/sample - loss: 0.0930 - acc: 1.0000 - val_loss: 0.1314 - val_acc: 0.8667\n",
      "Epoch 92/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 1.0000\n",
      "Epoch 00092: val_loss improved from 0.13056 to 0.13035, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0927 - acc: 1.0000 - val_loss: 0.1303 - val_acc: 0.8667\n",
      "Epoch 93/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 1.0000\n",
      "Epoch 00093: val_loss did not improve from 0.13035\n",
      "60/60 [==============================] - 7s 114ms/sample - loss: 0.0924 - acc: 1.0000 - val_loss: 0.1315 - val_acc: 0.8667\n",
      "Epoch 94/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9831\n",
      "Epoch 00094: val_loss improved from 0.13035 to 0.13027, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0921 - acc: 0.9833 - val_loss: 0.1303 - val_acc: 0.8667\n",
      "Epoch 95/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9831\n",
      "Epoch 00095: val_loss improved from 0.13027 to 0.12980, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.0916 - acc: 0.9833 - val_loss: 0.1298 - val_acc: 0.8667\n",
      "Epoch 96/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 1.0000\n",
      "Epoch 00096: val_loss improved from 0.12980 to 0.12894, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0911 - acc: 1.0000 - val_loss: 0.1289 - val_acc: 0.8667\n",
      "Epoch 97/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 1.0000\n",
      "Epoch 00097: val_loss did not improve from 0.12894\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0909 - acc: 1.0000 - val_loss: 0.1306 - val_acc: 0.8667\n",
      "Epoch 98/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 1.0000\n",
      "Epoch 00098: val_loss did not improve from 0.12894\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0903 - acc: 1.0000 - val_loss: 0.1303 - val_acc: 0.8667\n",
      "Epoch 99/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 1.0000\n",
      "Epoch 00099: val_loss did not improve from 0.12894\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0901 - acc: 1.0000 - val_loss: 0.1290 - val_acc: 0.8667\n",
      "Epoch 100/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 1.0000\n",
      "Epoch 00100: val_loss improved from 0.12894 to 0.12878, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.0897 - acc: 1.0000 - val_loss: 0.1288 - val_acc: 0.8667\n",
      "Epoch 101/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 1.0000\n",
      "Epoch 00101: val_loss improved from 0.12878 to 0.12857, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 127ms/sample - loss: 0.0892 - acc: 1.0000 - val_loss: 0.1286 - val_acc: 0.8667\n",
      "Epoch 102/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 1.0000\n",
      "Epoch 00102: val_loss improved from 0.12857 to 0.12820, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.0888 - acc: 1.0000 - val_loss: 0.1282 - val_acc: 0.8667\n",
      "Epoch 103/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 1.0000\n",
      "Epoch 00103: val_loss improved from 0.12820 to 0.12684, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.0884 - acc: 1.0000 - val_loss: 0.1268 - val_acc: 0.8667\n",
      "Epoch 104/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 1.0000\n",
      "Epoch 00104: val_loss improved from 0.12684 to 0.12640, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0880 - acc: 1.0000 - val_loss: 0.1264 - val_acc: 0.8667\n",
      "Epoch 105/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 1.0000\n",
      "Epoch 00105: val_loss improved from 0.12640 to 0.12615, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.0878 - acc: 1.0000 - val_loss: 0.1261 - val_acc: 0.8667\n",
      "Epoch 106/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 1.0000\n",
      "Epoch 00106: val_loss improved from 0.12615 to 0.12526, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0872 - acc: 1.0000 - val_loss: 0.1253 - val_acc: 0.8667\n",
      "Epoch 107/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 1.0000\n",
      "Epoch 00107: val_loss improved from 0.12526 to 0.12403, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.0869 - acc: 1.0000 - val_loss: 0.1240 - val_acc: 0.8667\n",
      "Epoch 108/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.12403\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.0865 - acc: 1.0000 - val_loss: 0.1250 - val_acc: 0.8667\n",
      "Epoch 109/175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/60 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 0.12403\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.0861 - acc: 1.0000 - val_loss: 0.1251 - val_acc: 0.8667\n",
      "Epoch 110/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 1.0000\n",
      "Epoch 00110: val_loss did not improve from 0.12403\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0858 - acc: 1.0000 - val_loss: 0.1244 - val_acc: 0.8667\n",
      "Epoch 111/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 1.0000\n",
      "Epoch 00111: val_loss improved from 0.12403 to 0.12363, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0853 - acc: 1.0000 - val_loss: 0.1236 - val_acc: 0.8667\n",
      "Epoch 112/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 0.12363\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0851 - acc: 1.0000 - val_loss: 0.1238 - val_acc: 0.8667\n",
      "Epoch 113/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 1.0000\n",
      "Epoch 00113: val_loss improved from 0.12363 to 0.12235, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0845 - acc: 1.0000 - val_loss: 0.1224 - val_acc: 0.8667\n",
      "Epoch 114/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.12235\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0842 - acc: 1.0000 - val_loss: 0.1235 - val_acc: 0.8667\n",
      "Epoch 115/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 1.0000\n",
      "Epoch 00115: val_loss did not improve from 0.12235\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.0838 - acc: 1.0000 - val_loss: 0.1229 - val_acc: 0.8667\n",
      "Epoch 116/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 1.0000\n",
      "Epoch 00116: val_loss did not improve from 0.12235\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0834 - acc: 1.0000 - val_loss: 0.1224 - val_acc: 0.8667\n",
      "Epoch 117/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 0.12235\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0831 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.8667\n",
      "Epoch 118/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 1.0000\n",
      "Epoch 00118: val_loss did not improve from 0.12235\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0828 - acc: 1.0000 - val_loss: 0.1229 - val_acc: 0.8667\n",
      "Epoch 119/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 1.0000\n",
      "Epoch 00119: val_loss improved from 0.12235 to 0.12202, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.0824 - acc: 1.0000 - val_loss: 0.1220 - val_acc: 0.8667\n",
      "Epoch 120/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 1.0000\n",
      "Epoch 00120: val_loss improved from 0.12202 to 0.12139, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0822 - acc: 1.0000 - val_loss: 0.1214 - val_acc: 0.8667\n",
      "Epoch 121/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 1.0000\n",
      "Epoch 00121: val_loss improved from 0.12139 to 0.12115, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 128ms/sample - loss: 0.0821 - acc: 1.0000 - val_loss: 0.1211 - val_acc: 0.8667\n",
      "Epoch 122/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 1.0000\n",
      "Epoch 00122: val_loss improved from 0.12115 to 0.12064, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.0821 - acc: 1.0000 - val_loss: 0.1206 - val_acc: 0.8667\n",
      "Epoch 123/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.12064\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.0820 - acc: 1.0000 - val_loss: 0.1213 - val_acc: 0.8667\n",
      "Epoch 124/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 1.0000\n",
      "Epoch 00124: val_loss improved from 0.12064 to 0.12045, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0819 - acc: 1.0000 - val_loss: 0.1205 - val_acc: 0.8667\n",
      "Epoch 125/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 1.0000\n",
      "Epoch 00125: val_loss did not improve from 0.12045\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0819 - acc: 1.0000 - val_loss: 0.1206 - val_acc: 0.8667\n",
      "Epoch 126/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 1.0000\n",
      "Epoch 00126: val_loss did not improve from 0.12045\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0818 - acc: 1.0000 - val_loss: 0.1208 - val_acc: 0.8667\n",
      "Epoch 127/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 1.0000\n",
      "Epoch 00127: val_loss improved from 0.12045 to 0.12017, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0817 - acc: 1.0000 - val_loss: 0.1202 - val_acc: 0.8667\n",
      "Epoch 128/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.12017\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.0816 - acc: 1.0000 - val_loss: 0.1204 - val_acc: 0.8667\n",
      "Epoch 129/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 1.0000- ETA: 0s - loss: 0.0726\n",
      "Epoch 00129: val_loss did not improve from 0.12017\n",
      "60/60 [==============================] - 7s 114ms/sample - loss: 0.0816 - acc: 1.0000 - val_loss: 0.1203 - val_acc: 0.8667\n",
      "Epoch 130/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 1.0000\n",
      "Epoch 00130: val_loss improved from 0.12017 to 0.12000, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.0815 - acc: 1.0000 - val_loss: 0.1200 - val_acc: 0.8667\n",
      "Epoch 131/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 1.0000\n",
      "Epoch 00131: val_loss did not improve from 0.12000\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0814 - acc: 1.0000 - val_loss: 0.1200 - val_acc: 0.8667\n",
      "Epoch 132/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 1.0000\n",
      "Epoch 00132: val_loss did not improve from 0.12000\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0813 - acc: 1.0000 - val_loss: 0.1200 - val_acc: 0.8667\n",
      "Epoch 133/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 1.0000\n",
      "Epoch 00133: val_loss did not improve from 0.12000\n",
      "60/60 [==============================] - 7s 110ms/sample - loss: 0.0812 - acc: 1.0000 - val_loss: 0.1201 - val_acc: 0.8667\n",
      "Epoch 134/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 1.0000\n",
      "Epoch 00134: val_loss did not improve from 0.12000\n",
      "60/60 [==============================] - 7s 111ms/sample - loss: 0.0812 - acc: 1.0000 - val_loss: 0.1201 - val_acc: 0.8667\n",
      "Epoch 135/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 1.0000\n",
      "Epoch 00135: val_loss improved from 0.12000 to 0.11998, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.0811 - acc: 1.0000 - val_loss: 0.1200 - val_acc: 0.8667\n",
      "Epoch 136/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 1.0000\n",
      "Epoch 00136: val_loss improved from 0.11998 to 0.11987, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.0810 - acc: 1.0000 - val_loss: 0.1199 - val_acc: 0.8667\n",
      "Epoch 137/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 1.0000\n",
      "Epoch 00137: val_loss improved from 0.11987 to 0.11979, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.0810 - acc: 1.0000 - val_loss: 0.1198 - val_acc: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 1.0000\n",
      "Epoch 00138: val_loss improved from 0.11979 to 0.11963, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.0809 - acc: 1.0000 - val_loss: 0.1196 - val_acc: 0.8667\n",
      "Epoch 139/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 1.0000- ETA: 0s - loss: 0.0864 - \n",
      "Epoch 00139: val_loss improved from 0.11963 to 0.11960, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.0809 - acc: 1.0000 - val_loss: 0.1196 - val_acc: 0.8667\n",
      "Epoch 140/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 1.0000\n",
      "Epoch 00140: val_loss did not improve from 0.11960\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.0808 - acc: 1.0000 - val_loss: 0.1198 - val_acc: 0.8667\n",
      "Epoch 141/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 1.0000- ETA\n",
      "Epoch 00141: val_loss did not improve from 0.11960\n",
      "60/60 [==============================] - 7s 113ms/sample - loss: 0.0808 - acc: 1.0000 - val_loss: 0.1197 - val_acc: 0.8667\n",
      "Epoch 142/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 1.0000- ETA: 0s - loss: 0.0860 - \n",
      "Epoch 00142: val_loss did not improve from 0.11960\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0808 - acc: 1.0000 - val_loss: 0.1196 - val_acc: 0.8667\n",
      "Epoch 143/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 1.0000- ETA: 0s - loss: 0.0867 - acc\n",
      "Epoch 00143: val_loss did not improve from 0.11960\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0807 - acc: 1.0000 - val_loss: 0.1196 - val_acc: 0.8667\n",
      "Epoch 144/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 1.0000\n",
      "Epoch 00144: val_loss improved from 0.11960 to 0.11955, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0807 - acc: 1.0000 - val_loss: 0.1195 - val_acc: 0.8667\n",
      "Epoch 145/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 1.0000\n",
      "Epoch 00145: val_loss improved from 0.11955 to 0.11948, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 129ms/sample - loss: 0.0807 - acc: 1.0000 - val_loss: 0.1195 - val_acc: 0.8667\n",
      "Epoch 146/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 1.0000- ETA: 3s - loss: 0.0786 - acc: 1. - ETA: 3s - loss: 0.0882 - acc: \n",
      "Epoch 00146: val_loss did not improve from 0.11948\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.0806 - acc: 1.0000 - val_loss: 0.1195 - val_acc: 0.8667\n",
      "Epoch 147/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 1.0000\n",
      "Epoch 00147: val_loss did not improve from 0.11948\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0806 - acc: 1.0000 - val_loss: 0.1196 - val_acc: 0.8667\n",
      "Epoch 148/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 1.0000\n",
      "Epoch 00148: val_loss improved from 0.11948 to 0.11940, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0806 - acc: 1.0000 - val_loss: 0.1194 - val_acc: 0.8667\n",
      "Epoch 149/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 1.0000\n",
      "Epoch 00149: val_loss did not improve from 0.11940\n",
      "60/60 [==============================] - 7s 114ms/sample - loss: 0.0805 - acc: 1.0000 - val_loss: 0.1194 - val_acc: 0.8667\n",
      "Epoch 150/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 1.0000\n",
      "Epoch 00150: val_loss improved from 0.11940 to 0.11938, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0805 - acc: 1.0000 - val_loss: 0.1194 - val_acc: 0.8667\n",
      "Epoch 151/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 1.0000\n",
      "Epoch 00151: val_loss improved from 0.11938 to 0.11929, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0804 - acc: 1.0000 - val_loss: 0.1193 - val_acc: 0.8667\n",
      "Epoch 152/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 1.0000\n",
      "Epoch 00152: val_loss improved from 0.11929 to 0.11921, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0804 - acc: 1.0000 - val_loss: 0.1192 - val_acc: 0.8667\n",
      "Epoch 153/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.11921\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0804 - acc: 1.0000 - val_loss: 0.1194 - val_acc: 0.8667\n",
      "Epoch 154/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 1.0000\n",
      "Epoch 00154: val_loss improved from 0.11921 to 0.11911, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0803 - acc: 1.0000 - val_loss: 0.1191 - val_acc: 0.8667\n",
      "Epoch 155/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 1.0000\n",
      "Epoch 00155: val_loss did not improve from 0.11911\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.0803 - acc: 1.0000 - val_loss: 0.1192 - val_acc: 0.8667\n",
      "Epoch 156/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 1.0000- ETA: 0s - loss: 0.0859 - acc:\n",
      "Epoch 00156: val_loss improved from 0.11911 to 0.11902, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0803 - acc: 1.0000 - val_loss: 0.1190 - val_acc: 0.8667\n",
      "Epoch 157/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 1.0000\n",
      "Epoch 00157: val_loss improved from 0.11902 to 0.11900, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.0802 - acc: 1.0000 - val_loss: 0.1190 - val_acc: 0.8667\n",
      "Epoch 158/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.11900\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0802 - acc: 1.0000 - val_loss: 0.1192 - val_acc: 0.8667\n",
      "Epoch 159/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 1.0000\n",
      "Epoch 00159: val_loss did not improve from 0.11900\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0801 - acc: 1.0000 - val_loss: 0.1190 - val_acc: 0.8667\n",
      "Epoch 160/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 1.0000\n",
      "Epoch 00160: val_loss improved from 0.11900 to 0.11895, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0801 - acc: 1.0000 - val_loss: 0.1190 - val_acc: 0.8667\n",
      "Epoch 161/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 1.0000\n",
      "Epoch 00161: val_loss did not improve from 0.11895\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0801 - acc: 1.0000 - val_loss: 0.1190 - val_acc: 0.8667\n",
      "Epoch 162/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.11895\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0800 - acc: 1.0000 - val_loss: 0.1190 - val_acc: 0.8667\n",
      "Epoch 163/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 1.0000\n",
      "Epoch 00163: val_loss improved from 0.11895 to 0.11878, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.0800 - acc: 1.0000 - val_loss: 0.1188 - val_acc: 0.8667\n",
      "Epoch 164/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 1.0000- ETA: 1s - loss: \n",
      "Epoch 00164: val_loss did not improve from 0.11878\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0800 - acc: 1.0000 - val_loss: 0.1189 - val_acc: 0.8667\n",
      "Epoch 165/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 1.0000\n",
      "Epoch 00165: val_loss did not improve from 0.11878\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.0799 - acc: 1.0000 - val_loss: 0.1190 - val_acc: 0.8667\n",
      "Epoch 166/175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/60 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.11878\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0799 - acc: 1.0000 - val_loss: 0.1188 - val_acc: 0.8667\n",
      "Epoch 167/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 1.0000\n",
      "Epoch 00167: val_loss improved from 0.11878 to 0.11867, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0798 - acc: 1.0000 - val_loss: 0.1187 - val_acc: 0.8667\n",
      "Epoch 168/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 1.0000\n",
      "Epoch 00168: val_loss did not improve from 0.11867\n",
      "60/60 [==============================] - 7s 114ms/sample - loss: 0.0798 - acc: 1.0000 - val_loss: 0.1187 - val_acc: 0.8667\n",
      "Epoch 169/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 1.0000\n",
      "Epoch 00169: val_loss did not improve from 0.11867\n",
      "60/60 [==============================] - 7s 115ms/sample - loss: 0.0798 - acc: 1.0000 - val_loss: 0.1187 - val_acc: 0.8667\n",
      "Epoch 170/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 1.0000\n",
      "Epoch 00170: val_loss improved from 0.11867 to 0.11865, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0797 - acc: 1.0000 - val_loss: 0.1186 - val_acc: 0.8667\n",
      "Epoch 171/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 1.0000\n",
      "Epoch 00171: val_loss improved from 0.11865 to 0.11859, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.0797 - acc: 1.0000 - val_loss: 0.1186 - val_acc: 0.8667\n",
      "Epoch 172/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 1.0000\n",
      "Epoch 00172: val_loss improved from 0.11859 to 0.11858, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 116ms/sample - loss: 0.0797 - acc: 1.0000 - val_loss: 0.1186 - val_acc: 0.8667\n",
      "Epoch 173/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 1.0000\n",
      "Epoch 00173: val_loss improved from 0.11858 to 0.11850, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.0796 - acc: 1.0000 - val_loss: 0.1185 - val_acc: 0.8667\n",
      "Epoch 174/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 1.0000\n",
      "Epoch 00174: val_loss improved from 0.11850 to 0.11846, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.0796 - acc: 1.0000 - val_loss: 0.1185 - val_acc: 0.8667\n",
      "Epoch 175/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.11846\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.0795 - acc: 1.0000 - val_loss: 0.1185 - val_acc: 0.8667\n"
     ]
    }
   ],
   "source": [
    "\"\"\" train the model \"\"\"\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "siamese.compile(loss=utils.loss(1), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# siamese.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "siamese.summary()\n",
    "history = siamese.fit([x_train_1, x_train_2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val_1, x_val_2], labels_val),\n",
    "    batch_size=1,\n",
    "    epochs=175,   # 175 for contrastive 100 for cross ent\n",
    "    callbacks = [checkpointer, early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cde2680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy\n",
    "utils.plt_metric(history=history.history, metric=\"acc\", title=\"Model accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b788b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the constrastive loss\n",
    "utils.plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9478b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296/1296 [==============================] - 52s 40ms/sample - loss: 0.0971 - acc: 0.9483\n",
      "test loss, test acc: [0.09711058033101352, 0.94830245]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test the model \"\"\"\n",
    "results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "67bdbdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40711525],\n",
       "       [0.1815041 ],\n",
       "       [0.62134373],\n",
       "       ...,\n",
       "       [0.05995859],\n",
       "       [0.5771121 ],\n",
       "       [0.04537573]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = siamese.predict([x_test_1, x_test_2]).squeeze()\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6e2d824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32110322"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c995cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       ...,\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_pred = Y_pred > Y_pred.mean()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "471bdd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = labels_test\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3591d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on test data\n",
      "Accuracy: 0.9768518518518519\n",
      "Precision: 0.9773065334299552\n",
      "Recall: 0.9768518518518519\n",
      "ROC AUC: 0.9768518518518517\n",
      "F1: 0.9768463378147898\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluate on test data\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1:\", f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "acf4298c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0778cdc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (2), usually from a call to set_ticks, does not match the number of ticklabels (1296).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-2f200d2a59e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcm_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspecificity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\metrics\\_plot\\confusion_matrix.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, include_values, cmap, xticks_rotation, values_format, ax)\u001b[0m\n\u001b[0;32m    107\u001b[0m                \u001b[0myticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                \u001b[0mylabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"True label\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                xlabel=\"Predicted label\")\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmove_color_to_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"color\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"color\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfindobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, props)\u001b[0m\n\u001b[0;32m    996\u001b[0m                         raise AttributeError(f\"{type(self).__name__!r} object \"\n\u001b[0;32m    997\u001b[0m                                              f\"has no property {k!r}\")\n\u001b[1;32m--> 998\u001b[1;33m                     \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpchanged\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m                 \u001b[1;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_set_ticklabels\u001b[1;34m(self, labels, fontdict, minor, **kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfontdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1796\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1798\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_keyword_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"3.2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"minor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mset_ticklabels\u001b[1;34m(self, ticklabels, minor, **kwargs)\u001b[0m\n\u001b[0;32m   1716\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1717\u001b[0m                 raise ValueError(\n\u001b[1;32m-> 1718\u001b[1;33m                     \u001b[1;34m\"The number of FixedLocator locations\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1719\u001b[0m                     \u001b[1;34mf\" ({len(locator.locs)}), usually from a call to\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1720\u001b[0m                     \u001b[1;34m\" set_ticks, does not match\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The number of FixedLocator locations (2), usually from a call to set_ticks, does not match the number of ticklabels (1296)."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAD4CAYAAABbu6u/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAYZ0lEQVR4nO3de5hU1Znv8e/bTdPc6Ybm0tIgKE0I5AleeJhM1MSMMaBhBs/McE7nTBIyYWIeDyEhiTpgYjyZiDonk6gzCtEEIzqOBBMZUJMow8TRmAjiiCKXlkawaW7N3QakL9Xv+aO2WoTuqt3pampX79/nefZTu1atWnuVyMta7157b3N3RESirCDXHRARyUSBSkQiT4FKRCJPgUpEIk+BSkQir0dXNFo2qNBHjyzqiqali7zxWp9cd0E64BQnaPJG60wbUz/R1w8dToSq+/JrjU+7+7TOHK8zuiRQjR5ZxLqnR3ZF09JFpp5zQa67IB2w1td0uo1DhxOse3pUqLqF5dvKOn3ATuiSQCUi0edAK6257kYoClQiMeU4zR5u6pdrClQiMaYRlYhEmuMk8uQSOgUqkRhrRYFKRCLMgYQClYhEnUZUIhJpDjQrRyUiUea4pn4iEnEOifyIU7ooWSSukivTw22ZmFmJmf3czLaa2RYz+1MzG2Rmq81sW/BamlJ/gZnVmFm1mU3N1L4ClUhsGYmQWwh3A7929/HAJGALMB9Y4+6VwJrgPWY2AagCJgLTgEVmVpiucQUqkZhKJtMt1JaOmQ0APgYsAXD3Jnc/CswAlgbVlgLXBPszgGXu3ujuO4AaYEq6YyhQicRUch1V6BFVmZmtT9muTWnqPOAA8FMze8XMfmJmfYFh7r4XIHgdGtQfAexK+X5dUNYuJdNFYqw1w2gpxUF3n9zOZz2Ai4C57r7WzO4mmOa1o62Dpk3ra0QlElMdHFGlUwfUufva4P3PSQau/WZWDhC81qfUT71hXQWwJ90BFKhEYsoxEhSE2tK2474P2GVmHwiKrgA2A6uAWUHZLGBlsL8KqDKzYjMbA1QC69IdQ1M/kRjrwNQvk7nAI2bWE3gT+FuSA6HlZjYbqAVmArj7JjNbTjKYtQBz3NPfGEuBSiSmHKPJ064KCN+W+wagrRzWFe3UXwgsDNu+ApVITCUXfOZH9keBSiTGQi7mzDkFKpGYcjcSrhGViERcq0ZUIhJlyWR6foSA/OiliGSdkukikhcS2VtH1aUUqERi6t2V6flAgUokxlp11k9Eoix5UbIClYhEmGM0Z+kSmq6mQCUSU+5owaeIRJ1pwaeIRJujEZWI5AEl00Uk0hzL5o3zupQClUhMJR+XlR8hID96KSJdIPTDRXNOgUokphytTBeRPKARlYhEmrtpRCUi0ZZMpusSGhGJNN0zXUQiLplMV45KRCIuX1am50cvRSTr3l2ZHmbLxMx2mtlGM9tgZuuDskFmttrMtgWvpSn1F5hZjZlVm9nUTO0rUInEWCsFobaQPuHuF7j7u492nw+scfdKYE3wHjObAFQBE4FpwCIzS5vVV6ASiSl3aG4tCLX9kWYAS4P9pcA1KeXL3L3R3XcANcCUdA0pUInEVHLqVxBqA8rMbH3Kdu0ZzcEzZvZyymfD3H0vQPA6NCgfAexK+W5dUNYuJdNFYqwDK9MPpkzp2nKJu+8xs6HAajPbmqZuWwf1dAfv1oHq+LFC7rx+JDu39sIMvvHDWiZMPnlGveoNvZk3fRw3/Wgnl00/1qljNjUa3//qKLZt7MOA0hZu+tFbDB/ZxPbXe/MvCyo40VBAYSFUfXU/l8842qljSfuWrt3MO8cLaW2FRIsx96pxue5S5GRzeYK77wle681sBcmp3H4zK3f3vWZWDtQH1euAkSlfrwD2pGs/1NTPzKYF2fkaM5vf4V+RI4u/M4LJl7/Nkue3svg/qhlV2XhGnUQCliw8h4svb+hQ2/t29eSGvxp7RvnTjw6iX0mCB3+3hb/80gGW3FoOQHHvVm64+y1+/Gw1Cx/Zzn23jOD4sfxYFZyvbpx5Pv/nyg8oSLWrQ1O/9lsx62tm/d/dBz4FvA6sAmYF1WYBK4P9VUCVmRWb2RigEliX7hgZR1RBNv5e4EqSkfAlM1vl7pszfTeXTjQUsPHFvlx/Vy0ART2dop6JM+qtfGAIl159jDde7XNa+ZpflPLvS8poaSpg/EUn+MrtdRSGiCu/f3ogn/3mPgAum36Ue79VgTtUnP9+kBw8vIWBZS0cO1RIv4Fn9knkbMnSPdOHASvMDJIx5d/c/ddm9hKw3MxmA7XATAB332Rmy4HNQAswx93T/kUIM/WbAtS4+5sAZraMZNY+0oFq31vFDBzcwg++Poo3N/Wi8sPvcN33dtOrT+t7dQ7uLeJ3vxrIPz5WwxvfHPVeee22Yv5rZQl3rtxGjyL4lwUV/OfjpVw580jG4x7cV8SQc5oBKOwBfQckePtwIQMHv//nsPWVPrQ0GeWjm7L4i+U0btz26Jvg8NTDg/nVI4Nz3aPISZ716/yoPogNk9ooPwRc0c53FgILwx4jTKBqK0P/J39YKcj0XwswakTuU1+JBNRs7MOcW3cz/qKTLL55BD+7Zyizbtz3Xp0f3TKC2d/ac8ZI6ZXn+7NtYx/mXvUBAJpOGSWDWwD47hdHs6+2mJZmo353Edd9Mlnnmr87wNSqw3gbKUFL+Ufr0P4efH/uKK6/u5YCnXPtMl+fMZbD+4sYOLiZO5a9ya6aYl5f2y/X3YqU7nYr4lAZene/H7gfYPKkXmkz+GdDWXkzQ8qbGX9RMnl+6fSjLL9n6Gl13ni1N7dfNxqAY4cLWbemfzJoOVw58zBfvGnvGe3e8sBOIJmj+sG8UXz/FzWnfT6kvJkDe5KjqkQLnHi7kP6lydHUiYYCvvO585j193v54MVnJvUlew7vLwLg2KEiXvj1QMZfeFKBqg358risMP+mdzhDHwWDhrZQdk4Tu2qKAdjwfP8zkukPrd3CQ+s289C6zVw2/Rhzb6/jo1cd44LLGnj+qRKOHkzG8bePFLK/rijUcT/yqbdZ/dggAJ5/soRJlzZgBs1Nxj/MHsMVM4/wsT/v3JlFSa+4d4LefRPv7V/88QZ2bu2V415Fz7tn/bJxCU1XCzOiegmoDLLzu0kuff/fXdqrLJlz627+8Svn0tJsDB/VxDfvrOXJh5K5iumfP9Tu984d18isG/eyoOp83KGwh/OV2+oYVtGc8ZjTPnOI//fVc/nCRz9I/5IWblr8FgDPPVHCxhf78fbhHqz+WTKQXX9XLed/6J0s/FJJVTqkhVuW7ASSf3a/WVHK+mcH5LZTEZUvN84zbyup8oeVzK4G7gIKgQeCRFi7Jk/q5eueHpmuikTM1HMuyHUXpAPW+hre9sOdGuqUjh/qf/bAX4eq+/gli1/OsOCzS4XKerv7L4FfdnFfROQsi8K0Lozcn54TkZzQjfNEJC8oUIlIpHW3dVQi0k3lyzoqBSqRmHKHlj/+pnhnlQKVSIxp6icikaYclYjkBVegEpGoUzJdRCLNXTkqEYk8I6GzfiISdcpRiUik6Vo/EYk+p81bZ0eRApVIjOmsn4hEmiuZLiL5QFM/EYk8nfUTkUhzz59AlR8TVBHpEtl8XJaZFZrZK2b2ZPB+kJmtNrNtwWtpSt0FZlZjZtVmNjVT2wpUIjHmHm4L6WvAlpT384E17l4JrAneY2YTSD52byIwDVhkZmmfLa9AJRJTjtHaWhBqy8TMKoBPAz9JKZ4BLA32lwLXpJQvc/dGd98B1ABT0rWvQCUSYx5yA8rMbH3Kdu0fNHUXcCPQmlI2zN33AgSvQ4PyEcCulHp1QVm7lEwXiauOJdMPtvcAUjObDtS7+8tmdnmItto6aNoJpgKVSJxlZx3VJcBfBE9U7wUMMLN/BfabWbm77zWzcqA+qF8HpD5KvQLYk+4AmvqJxJi7hdrSt+EL3L3C3UeTTJL/p7t/FlgFzAqqzQJWBvurgCozKzazMUAlsC7dMTSiEokpB1pbu3Qd1R3AcjObDdQCMwHcfZOZLQc2Ay3AHHdPpGtIgUokrhzI8oJPd38WeDbYPwRc0U69hcDCsO0qUInEmK71E5HoU6ASkWjLnCiPCgUqkTjTiEpEIs3Bu/asX9YoUInEmgKViESdpn4iEnkKVCISaV2w4LOrKFCJxJgWfIpI9Omsn4hEnWlEJSKRlnL7zqhToBKJLVMyXUTygEZUIhJ5rZmrRIEClUhcaR2ViOQDnfUTkejLk0Clp9CISOR1yYhq2+v9uKrykq5oWrrII7ueyXUXpAM+dfXxrLSjqZ+IRJujS2hEJA9oRCUiUaepn4hEX54EKp31E4kzD7mlYWa9zGydmb1qZpvM7LtB+SAzW21m24LX0pTvLDCzGjOrNrOpmbqpQCUSU+bhtwwagT9z90nABcA0M/sIMB9Y4+6VwJrgPWY2AagCJgLTgEVmVpjuAApUInHWauG2NDzp3fUSRcHmwAxgaVC+FLgm2J8BLHP3RnffAdQAU9IdQ4FKJMY6MKIqM7P1Kdu1p7VjVmhmG4B6YLW7rwWGuftegOB1aFB9BLAr5et1QVm7lEwXibPwyfSD7j653WbcE8AFZlYCrDCzD6Vpq60hWtqeaEQlElfZy1G936T7UeBZkrmn/WZWDhC81gfV6oCRKV+rAPaka1eBSiTOsnPWb0gwksLMegOfBLYCq4BZQbVZwMpgfxVQZWbFZjYGqATWpTuGpn4iMWbZuXFeObA0OHNXACx39yfN7PfAcjObDdQCMwHcfZOZLQc2Ay3AnGDq2C4FKhHpFHd/DbiwjfJDwBXtfGchsDDsMRSoROIsT1amK1CJxFUHE+W5pEAlEmcKVCISeQpUIhJlRtbO+nU5BSqRuFKOSkTyggKViESeApWIRJ2mfiISfQpUIhJprrN+IpIPNKISkahTjkpEok+BSkQiLcRN8aJCgUokpgxN/UQkDyhQiUj0KVCJSOQpUIlIpOnuCSKSFxSoRCTqdAmNiESepn4iEm1a8CkieSFPAlVBrjsgIrnx7sr0MFvadsxGmtlvzGyLmW0ys68F5YPMbLWZbQteS1O+s8DMasys2symZuqrApVIjFmrh9oyaAG+6e4fBD4CzDGzCcB8YI27VwJrgvcEn1UBE4FpwCIzK0x3AAUqkbjyDmzpmnHf6+7/Hew3AFuAEcAMYGlQbSlwTbA/A1jm7o3uvgOoAaakO4YClUiMdWDqV2Zm61O2a9tsz2w0cCGwFhjm7nshGcyAoUG1EcCulK/VBWXtUjJdJM7CJ9MPuvvkdBXMrB/wC2Ceu79tZu1W7WhPNKISibFsJNMBzKyIZJB6xN0fD4r3m1l58Hk5UB+U1wEjU75eAexJ174ClUicZSFHZcmh0xJgi7v/MOWjVcCsYH8WsDKlvMrMis1sDFAJrEt3DE39ROIqe0+huQT4HLDRzDYEZTcBdwDLzWw2UAvMBHD3TWa2HNhM8ozhHHdPpDuAApVITGXrDp/u/lvazjsBXNHOdxYCC8MeQ4FKJM48P5amK1CJxJguSs5jZcMbuf772ygd0oy3wq9+NoyVS8/hb+bWMu1/1nPsSPI/29IfnMtL/1WaoTXpiBPHCvnxjWOpq+6DGVz7TzVUXtzw3ucvrBjCE4uSS2569U3wt7dt59wJJzt1zOZGY/G8cezc2Jd+pS3MXVTNkJGN7NzUl5/edB7vHO9BQYEzY24df/oXBzt1rEjpThclm9kDwHSg3t0/1PVdyr1Ewvjx7aPZvrkfvfsm+OcVr/LKCyUA/PuD5fxiSdq1adIJD//f85h0+VHm3VdNS5PR+M7pJ6aHjDzFzY9tpG9Jgg2/KWHJ34/lH554LVTbB3YVc983Kvn2Y6+fVv7ssmH0LWnhh7/9b36/soxHbxvNVxdXU9w7wXV3bWP4mFMc2deTb396Eh/++BH6Dkyb980r+XI/qjDLEx4keT1ObBw50JPtm/sB8M6JQnZt783gYU057lX3d7KhkK1rB3B51X4AevT0M4LCuMkN9C1JllVe2MDhvT3f++y3jw/h5ukfZsHUSSyZfz6tIePJy88M4mN/nVziM+XTB9n0wkDcofy8UwwfcwqA0uFNDBjcTMPhos7+zEix1nBbrmUMVO7+HHD4LPQlkoaOOMX5E05Q/WoycP35Z/ex6IkNfP32GvoNaMlx77qX+tpe9B/UzH3fGMtN0ybx4xvGcupk+/+LPrtsGJM+cRSA3dt68+ITZdyyYiO3P/0qBQXOCyuGhDrukX09GXROIwCFPaBP/xaOHzl9srH9lX60NBtDzz31x/24KHKSyfQwW45lLUcVXPtzLUAv65utZnOqV58E376nmvsWjuHk8R489W/DefTekbjD5+fV8qUFO7lzwdhcd7PbaG0xdr7ej1nfe5OxFx7noVvG8MS9Fcy8ofaMupt+N5BnfzaM7zy+Mfn+hYHseK0fN0//MADNpwoZUNYMwJ1/N576XcW0NBdwaHcxC6ZOAmDaF/fy8f9Vj7d1Zj2l6Mj+IhbPG8eX73yDgm62RDp2yXR3vx+4H2BgYVme/Pz2FfZo5dv3VPObVUP43TODATh66P1pxq+WD+O792/JVfe6pUHljQwqb2TshccBmHL1ofcS56lqt/ThJzecz40Pb6Z/aXJU625cNrOeqvlvnVH/6z/ZCrSfoxo0vJHDe4oZXN5EogVONvSgX0my3ZMNhfzTFyYw84a3qLzoeFZ/byTkyd/UbvbvQ7Y4827bzq7tvVnx03PeKy0d8n6e6qNXHuatN/rkonPdVsnQZgaXN7Jne28gOUoaUfnOaXUO7u7JXV8az3V3b6P8vPenYRMvOcq6pwZz7GAyh3T8SA8O1BWHOu5FVx7muZ8nL+xf91QZEy85hhm0NBl3fWk8l/5VPX8y/VA2fmKkZOvGeWeDlie0YeLFDXzyfxxgx9Y+3LNqA5BcivDx6Qc574MnwGH/7mL++ebzc9vRbujz39vBornjkvmgUaf48g+28R8PDwfgk5/bx4q7RtFwtIiffus8AAoL4dZfvkrFuHeYeUMtd/zNBLzVKCxyvnDrdoZUNGY85uVV+1k8bxzfuPQi+pa0MPfeagBefLKMrWsH0HCkB889lgxkX/5hDaMnnuiiX3+Weaib4kWCeYZEmZk9ClwOlAH7gVvcfUm67wwsLPOP9JmerT7KWfDw1mdy3QXpgE9dfZANrza1ex+VMPqXVPiFH/taqLrPP3Hjy5lu89KVMo6o3P0zZ6MjInL2RWFaF4amfiJx5UCeTP0UqETiLD/ilAKVSJxp6icikZcvZ/0UqETiqjvdPUFEuqfkgs/8iFQKVCJxFoE7I4ShQCUSYxpRiUi0KUclItGXP9f6KVCJxJmmfiISadl7AGmXU6ASibM8GVHpxnkiceYhtwzM7AEzqzez11PKBpnZajPbFryWpny2wMxqzKzazKZmal+BSiTGrLU11BbCg5z5tKr5wBp3rwTWBO8xswlAFTAx+M4iMytM17gClUhcOckFn2G2TE21/bSqGcDSYH8pcE1K+TJ3b3T3HUANMCVd+8pRicSU4R1Z8FlmZutT3t8fPNAlnWHuvhfA3fea2dCgfATwYkq9uqCsXQpUInEWPlAdzOKtiNu6hXLajmjqJxJnXfsA0v1mVg4QvNYH5XXAyJR6FcCedA0pUInEVRZzVO1YBcwK9mcBK1PKq8ys2MzGAJXAunQNaeonEmMhz+hlbiflaVVmVgfcAtwBLDez2UAtMBPA3TeZ2XJgM9ACzHH3RLr2FahEYqtT07rTW2r/aVVXtFN/IbAwbPsKVCJx5eTNynQFKpE407V+IhJ1unGeiESfApWIRJo7JPJj7qdAJRJnGlGJSOQpUIlIpDmge6aLSLQ5uHJUIhJljpLpIpIHlKMSkchToBKRaMveRcldTYFKJK4cyNJtXrqaApVInGlEJSLRpktoRCTqHFzrqEQk8rQyXUQiTzkqEYk0d531E5E8oBGViESb44m0T6mKDAUqkbjSbV5EJC9oeYKIRJkDrhGViESa68Z5IpIH8iWZbt4FpyfN7ADwVtYbzr0y4GCuOyEd0l3/zM519yGdacDMfk3yv08YB919WmeO1xldEqi6KzNb7+6Tc90PCU9/Zt1DQa47ICKSiQKViESeAlXH3J/rDkiH6c+sG1COSkQiTyMqEYk8BSoRiTwFqhDMbJqZVZtZjZnNz3V/JDMze8DM6s3s9Vz3RTpPgSoDMysE7gWuAiYAnzGzCbntlYTwIJCzBYqSXQpUmU0Batz9TXdvApYBM3LcJ8nA3Z8DDue6H5IdClSZjQB2pbyvC8pE5CxRoMrM2ijTmg6Rs0iBKrM6YGTK+wpgT476IhJLClSZvQRUmtkYM+sJVAGrctwnkVhRoMrA3VuArwBPA1uA5e6+Kbe9kkzM7FHg98AHzKzOzGbnuk/yx9MlNCISeRpRiUjkKVCJSOQpUIlI5ClQiUjkKVCJSOQpUIlI5ClQiUjk/X9onLqRfN28sAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)    \n",
    "# cm_display = ConfusionMatrixDisplay(cm, labels_test).plot()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e381e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
