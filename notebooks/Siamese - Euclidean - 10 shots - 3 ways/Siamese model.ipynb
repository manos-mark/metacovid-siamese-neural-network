{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9bc068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0927a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\t• TensorFlow version: 1.15.5\n",
      "\t• tf.keras version: 2.2.4-tf\n",
      "\t• Running on GPU\n"
     ]
    }
   ],
   "source": [
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "print('Using:')\n",
    "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
    "print('\\t\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f60667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images belonging to 3 classes.\n",
      "The train set contains 30\n",
      "Found 30 images belonging to 3 classes.\n",
      "The valid set contains 30\n",
      "Found 648 images belonging to 3 classes.\n",
      "The test set contains 648\n"
     ]
    }
   ],
   "source": [
    "basedir = os.path.join(\"C:\\\\Users\\\\manos\\\\git\\\\metacovid-siamese-neural-network\", \"dataset\", \"siamese\") \n",
    "\n",
    "train_image_list, train_y_list = utils.load_images(basedir, 'train', (100,100))\n",
    "print(\"The train set contains\",len(train_image_list)) \n",
    "\n",
    "valid_image_list, valid_y_list = utils.load_images(basedir, 'validation', (100,100))   \n",
    "print(\"The valid set contains\", len(valid_image_list))  \n",
    "\n",
    "test_image_list, test_y_list = utils.load_images(basedir, 'test', (100,100))   \n",
    "print(\"The test set contains\", len(test_image_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a873fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairs for training 60\n",
      "number of pairs for validation 60\n",
      "number of pairs for test 1296\n"
     ]
    }
   ],
   "source": [
    "# make train pairs\n",
    "pairs_train, labels_train = utils.make_pairs(train_image_list, train_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_val, labels_val = utils.make_pairs(valid_image_list, valid_y_list)\n",
    "\n",
    "# make test pairs\n",
    "pairs_test, labels_test = utils.make_pairs(test_image_list, test_y_list)\n",
    "\n",
    "x_train_1 = pairs_train[:, 0]  \n",
    "x_train_2 = pairs_train[:, 1]\n",
    "print(\"number of pairs for training\", np.shape(x_train_1)[0]) \n",
    "\n",
    "x_val_1 = pairs_val[:, 0] \n",
    "x_val_2 = pairs_val[:, 1]\n",
    "print(\"number of pairs for validation\", np.shape(x_val_1)[0]) \n",
    "\n",
    "x_test_1 = pairs_test[:, 0] \n",
    "x_test_2 = pairs_test[:, 1]\n",
    "print(\"number of pairs for test\", np.shape(x_test_1)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9dade0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 5120)         14748995    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 14,748,997\n",
      "Trainable params: 20,482\n",
      "Non-trainable params: 14,728,515\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "SIAMESE_MODEL_FNAME = 'siamese_network.h5'\n",
    "EMBEDDING_MODEL_FNAME = 'embedding_network.h5'\n",
    "\n",
    "input_1 = Input((100,100,3))\n",
    "input_2 = Input((100,100,3))\n",
    "\n",
    "embedding_network = tf.keras.models.load_model(EMBEDDING_MODEL_FNAME)\n",
    "embedding_network.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential() \n",
    "for layer in embedding_network.layers:  \n",
    "    model.add(layer) \n",
    "\n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dense(5120, name='den', activation='sigmoid', kernel_regularizer='l2')) \n",
    " \n",
    "output_1 = model(input_1) \n",
    "output_2 = model(input_2) \n",
    " \n",
    "merge_layer = Lambda(utils.euclidean_distance)([output_1, output_2]) \n",
    "output_layer = Dense(1, activation=\"sigmoid\")(merge_layer) \n",
    "siamese = Model(inputs=[input_1, input_2], outputs=output_layer) \n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b4a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" callbacks \"\"\"\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.0001)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='siamese_network.h5', verbose=1, \n",
    "                                save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b21af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 5120)         14748995    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 14,748,997\n",
      "Trainable params: 20,482\n",
      "Non-trainable params: 14,728,515\n",
      "__________________________________________________________________________________________________\n",
      "Train on 60 samples, validate on 60 samples\n",
      "Epoch 1/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.6780\n",
      "Epoch 00001: val_loss improved from inf to 0.25645, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 9s 155ms/sample - loss: 0.2537 - acc: 0.6833 - val_loss: 0.2565 - val_acc: 0.7167\n",
      "Epoch 2/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.7458\n",
      "Epoch 00002: val_loss improved from 0.25645 to 0.24909, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2439 - acc: 0.7500 - val_loss: 0.2491 - val_acc: 0.7167\n",
      "Epoch 3/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.8475\n",
      "Epoch 00003: val_loss improved from 0.24909 to 0.24441, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2386 - acc: 0.8500 - val_loss: 0.2444 - val_acc: 0.7333\n",
      "Epoch 4/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.8644\n",
      "Epoch 00004: val_loss improved from 0.24441 to 0.24196, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2355 - acc: 0.8667 - val_loss: 0.2420 - val_acc: 0.7500\n",
      "Epoch 5/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.8644\n",
      "Epoch 00005: val_loss improved from 0.24196 to 0.24052, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 131ms/sample - loss: 0.2338 - acc: 0.8667 - val_loss: 0.2405 - val_acc: 0.7500\n",
      "Epoch 6/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.8644\n",
      "Epoch 00006: val_loss improved from 0.24052 to 0.23968, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2330 - acc: 0.8667 - val_loss: 0.2397 - val_acc: 0.7667\n",
      "Epoch 7/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.8644\n",
      "Epoch 00007: val_loss improved from 0.23968 to 0.23921, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2325 - acc: 0.8667 - val_loss: 0.2392 - val_acc: 0.7667\n",
      "Epoch 8/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.8814\n",
      "Epoch 00008: val_loss improved from 0.23921 to 0.23868, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2322 - acc: 0.8833 - val_loss: 0.2387 - val_acc: 0.7667\n",
      "Epoch 9/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.8983\n",
      "Epoch 00009: val_loss did not improve from 0.23868\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2319 - acc: 0.9000 - val_loss: 0.2387 - val_acc: 0.7667\n",
      "Epoch 10/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9153\n",
      "Epoch 00010: val_loss improved from 0.23868 to 0.23840, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2318 - acc: 0.9000 - val_loss: 0.2384 - val_acc: 0.7667\n",
      "Epoch 11/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.8983\n",
      "Epoch 00011: val_loss improved from 0.23840 to 0.23822, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2317 - acc: 0.9000 - val_loss: 0.2382 - val_acc: 0.7667\n",
      "Epoch 12/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.8983\n",
      "Epoch 00012: val_loss improved from 0.23822 to 0.23811, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.2315 - acc: 0.9000 - val_loss: 0.2381 - val_acc: 0.7667\n",
      "Epoch 13/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9153\n",
      "Epoch 00013: val_loss improved from 0.23811 to 0.23802, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 127ms/sample - loss: 0.2315 - acc: 0.9000 - val_loss: 0.2380 - val_acc: 0.7667\n",
      "Epoch 14/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.8983\n",
      "Epoch 00014: val_loss improved from 0.23802 to 0.23793, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 128ms/sample - loss: 0.2312 - acc: 0.9000 - val_loss: 0.2379 - val_acc: 0.7667\n",
      "Epoch 15/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.8983\n",
      "Epoch 00015: val_loss improved from 0.23793 to 0.23780, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.2311 - acc: 0.9000 - val_loss: 0.2378 - val_acc: 0.7667\n",
      "Epoch 16/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.8983\n",
      "Epoch 00016: val_loss improved from 0.23780 to 0.23770, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.2310 - acc: 0.9000 - val_loss: 0.2377 - val_acc: 0.7667\n",
      "Epoch 17/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.8983\n",
      "Epoch 00017: val_loss improved from 0.23770 to 0.23768, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2309 - acc: 0.9000 - val_loss: 0.2377 - val_acc: 0.7667\n",
      "Epoch 18/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.8983\n",
      "Epoch 00018: val_loss improved from 0.23768 to 0.23746, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 126ms/sample - loss: 0.2308 - acc: 0.9000 - val_loss: 0.2375 - val_acc: 0.8000\n",
      "Epoch 19/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9153\n",
      "Epoch 00019: val_loss improved from 0.23746 to 0.23745, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2306 - acc: 0.9000 - val_loss: 0.2375 - val_acc: 0.7833\n",
      "Epoch 20/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9153\n",
      "Epoch 00020: val_loss improved from 0.23745 to 0.23730, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.2305 - acc: 0.9167 - val_loss: 0.2373 - val_acc: 0.8000\n",
      "Epoch 21/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.8983- ETA: 0s - loss: 0.2311\n",
      "Epoch 00021: val_loss improved from 0.23730 to 0.23721, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 131ms/sample - loss: 0.2302 - acc: 0.9000 - val_loss: 0.2372 - val_acc: 0.8000\n",
      "Epoch 22/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.8983\n",
      "Epoch 00022: val_loss improved from 0.23721 to 0.23706, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 128ms/sample - loss: 0.2301 - acc: 0.9000 - val_loss: 0.2371 - val_acc: 0.7833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9153\n",
      "Epoch 00023: val_loss improved from 0.23706 to 0.23706, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2299 - acc: 0.9167 - val_loss: 0.2371 - val_acc: 0.7833\n",
      "Epoch 24/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9492- ETA: 0s - loss: 0.2306 - acc: 0.94\n",
      "Epoch 00024: val_loss improved from 0.23706 to 0.23695, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2299 - acc: 0.9500 - val_loss: 0.2370 - val_acc: 0.7833\n",
      "Epoch 25/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.948 - ETA: 0s - loss: 0.2303 - acc: 0.9492\n",
      "Epoch 00025: val_loss improved from 0.23695 to 0.23683, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2297 - acc: 0.9500 - val_loss: 0.2368 - val_acc: 0.7833\n",
      "Epoch 26/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9492\n",
      "Epoch 00026: val_loss improved from 0.23683 to 0.23672, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2296 - acc: 0.9500 - val_loss: 0.2367 - val_acc: 0.7833\n",
      "Epoch 27/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9492\n",
      "Epoch 00027: val_loss improved from 0.23672 to 0.23667, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2294 - acc: 0.9500 - val_loss: 0.2367 - val_acc: 0.7833\n",
      "Epoch 28/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9492\n",
      "Epoch 00028: val_loss improved from 0.23667 to 0.23649, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2293 - acc: 0.9500 - val_loss: 0.2365 - val_acc: 0.7833\n",
      "Epoch 29/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9492\n",
      "Epoch 00029: val_loss improved from 0.23649 to 0.23641, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 129ms/sample - loss: 0.2291 - acc: 0.9500 - val_loss: 0.2364 - val_acc: 0.7833\n",
      "Epoch 30/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9492- ETA:\n",
      "Epoch 00030: val_loss did not improve from 0.23641\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2290 - acc: 0.9500 - val_loss: 0.2364 - val_acc: 0.7833\n",
      "Epoch 31/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9492\n",
      "Epoch 00031: val_loss improved from 0.23641 to 0.23624, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 129ms/sample - loss: 0.2288 - acc: 0.9500 - val_loss: 0.2362 - val_acc: 0.7833\n",
      "Epoch 32/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9492\n",
      "Epoch 00032: val_loss improved from 0.23624 to 0.23614, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 125ms/sample - loss: 0.2288 - acc: 0.9500 - val_loss: 0.2361 - val_acc: 0.7833\n",
      "Epoch 33/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9492\n",
      "Epoch 00033: val_loss improved from 0.23614 to 0.23603, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2286 - acc: 0.9500 - val_loss: 0.2360 - val_acc: 0.7833\n",
      "Epoch 34/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9492\n",
      "Epoch 00034: val_loss improved from 0.23603 to 0.23591, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2285 - acc: 0.9500 - val_loss: 0.2359 - val_acc: 0.7833\n",
      "Epoch 35/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9492\n",
      "Epoch 00035: val_loss did not improve from 0.23591\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2283 - acc: 0.9500 - val_loss: 0.2360 - val_acc: 0.7833\n",
      "Epoch 36/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9492\n",
      "Epoch 00036: val_loss improved from 0.23591 to 0.23572, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.2281 - acc: 0.9500 - val_loss: 0.2357 - val_acc: 0.8000\n",
      "Epoch 37/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9492\n",
      "Epoch 00037: val_loss improved from 0.23572 to 0.23568, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2280 - acc: 0.9500 - val_loss: 0.2357 - val_acc: 0.8000\n",
      "Epoch 38/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9492\n",
      "Epoch 00038: val_loss improved from 0.23568 to 0.23547, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2279 - acc: 0.9500 - val_loss: 0.2355 - val_acc: 0.8000\n",
      "Epoch 39/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9492\n",
      "Epoch 00039: val_loss improved from 0.23547 to 0.23546, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2277 - acc: 0.9500 - val_loss: 0.2355 - val_acc: 0.8000\n",
      "Epoch 40/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9492\n",
      "Epoch 00040: val_loss improved from 0.23546 to 0.23532, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2276 - acc: 0.9500 - val_loss: 0.2353 - val_acc: 0.8167\n",
      "Epoch 41/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9492\n",
      "Epoch 00041: val_loss improved from 0.23532 to 0.23528, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2274 - acc: 0.9500 - val_loss: 0.2353 - val_acc: 0.8167\n",
      "Epoch 42/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9492\n",
      "Epoch 00042: val_loss improved from 0.23528 to 0.23515, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2272 - acc: 0.9500 - val_loss: 0.2351 - val_acc: 0.8333\n",
      "Epoch 43/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9492\n",
      "Epoch 00043: val_loss improved from 0.23515 to 0.23503, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2270 - acc: 0.9500 - val_loss: 0.2350 - val_acc: 0.8500\n",
      "Epoch 44/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9492\n",
      "Epoch 00044: val_loss improved from 0.23503 to 0.23488, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2270 - acc: 0.9500 - val_loss: 0.2349 - val_acc: 0.8500\n",
      "Epoch 45/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.948 - ETA: 0s - loss: 0.2275 - acc: 0.9492\n",
      "Epoch 00045: val_loss improved from 0.23488 to 0.23488, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2269 - acc: 0.9500 - val_loss: 0.2349 - val_acc: 0.8500\n",
      "Epoch 46/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9492\n",
      "Epoch 00046: val_loss improved from 0.23488 to 0.23474, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2267 - acc: 0.9500 - val_loss: 0.2347 - val_acc: 0.8500\n",
      "Epoch 47/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9492\n",
      "Epoch 00047: val_loss improved from 0.23474 to 0.23459, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2264 - acc: 0.9500 - val_loss: 0.2346 - val_acc: 0.8500\n",
      "Epoch 48/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9492\n",
      "Epoch 00048: val_loss improved from 0.23459 to 0.23450, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2263 - acc: 0.9500 - val_loss: 0.2345 - val_acc: 0.8500\n",
      "Epoch 49/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9492\n",
      "Epoch 00049: val_loss improved from 0.23450 to 0.23450, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2262 - acc: 0.9500 - val_loss: 0.2345 - val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9492\n",
      "Epoch 00050: val_loss improved from 0.23450 to 0.23431, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2260 - acc: 0.9500 - val_loss: 0.2343 - val_acc: 0.8500\n",
      "Epoch 51/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9492\n",
      "Epoch 00051: val_loss improved from 0.23431 to 0.23429, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2259 - acc: 0.9500 - val_loss: 0.2343 - val_acc: 0.8500\n",
      "Epoch 52/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9492- ETA: 0s - loss: 0.2249 - acc: \n",
      "Epoch 00052: val_loss improved from 0.23429 to 0.23411, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2257 - acc: 0.9500 - val_loss: 0.2341 - val_acc: 0.8500\n",
      "Epoch 53/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9492\n",
      "Epoch 00053: val_loss improved from 0.23411 to 0.23394, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2256 - acc: 0.9500 - val_loss: 0.2339 - val_acc: 0.8667\n",
      "Epoch 54/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9492\n",
      "Epoch 00054: val_loss improved from 0.23394 to 0.23386, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2254 - acc: 0.9500 - val_loss: 0.2339 - val_acc: 0.8667\n",
      "Epoch 55/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9492\n",
      "Epoch 00055: val_loss improved from 0.23386 to 0.23379, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 125ms/sample - loss: 0.2252 - acc: 0.9500 - val_loss: 0.2338 - val_acc: 0.8667\n",
      "Epoch 56/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9492\n",
      "Epoch 00056: val_loss improved from 0.23379 to 0.23363, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2250 - acc: 0.9500 - val_loss: 0.2336 - val_acc: 0.8667\n",
      "Epoch 57/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9492\n",
      "Epoch 00057: val_loss did not improve from 0.23363\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2249 - acc: 0.9500 - val_loss: 0.2336 - val_acc: 0.8667\n",
      "Epoch 58/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9492\n",
      "Epoch 00058: val_loss improved from 0.23363 to 0.23347, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2248 - acc: 0.9500 - val_loss: 0.2335 - val_acc: 0.8833\n",
      "Epoch 59/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9492\n",
      "Epoch 00059: val_loss improved from 0.23347 to 0.23332, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2246 - acc: 0.9500 - val_loss: 0.2333 - val_acc: 0.9000\n",
      "Epoch 60/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9492\n",
      "Epoch 00060: val_loss improved from 0.23332 to 0.23329, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2244 - acc: 0.9500 - val_loss: 0.2333 - val_acc: 0.8833\n",
      "Epoch 61/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9492\n",
      "Epoch 00061: val_loss improved from 0.23329 to 0.23307, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2244 - acc: 0.9500 - val_loss: 0.2331 - val_acc: 0.9000\n",
      "Epoch 62/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9492\n",
      "Epoch 00062: val_loss improved from 0.23307 to 0.23303, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2242 - acc: 0.9500 - val_loss: 0.2330 - val_acc: 0.9000\n",
      "Epoch 63/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9492\n",
      "Epoch 00063: val_loss improved from 0.23303 to 0.23296, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2240 - acc: 0.9500 - val_loss: 0.2330 - val_acc: 0.9000\n",
      "Epoch 64/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9492\n",
      "Epoch 00064: val_loss improved from 0.23296 to 0.23278, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2238 - acc: 0.9500 - val_loss: 0.2328 - val_acc: 0.9000\n",
      "Epoch 65/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9661\n",
      "Epoch 00065: val_loss did not improve from 0.23278\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.2237 - acc: 0.9667 - val_loss: 0.2328 - val_acc: 0.9000\n",
      "Epoch 66/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9661\n",
      "Epoch 00066: val_loss improved from 0.23278 to 0.23263, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2235 - acc: 0.9667 - val_loss: 0.2326 - val_acc: 0.9000\n",
      "Epoch 67/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9661\n",
      "Epoch 00067: val_loss improved from 0.23263 to 0.23241, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2234 - acc: 0.9667 - val_loss: 0.2324 - val_acc: 0.9000\n",
      "Epoch 68/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9661\n",
      "Epoch 00068: val_loss did not improve from 0.23241\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2233 - acc: 0.9667 - val_loss: 0.2325 - val_acc: 0.9000\n",
      "Epoch 69/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9831\n",
      "Epoch 00069: val_loss improved from 0.23241 to 0.23227, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.2230 - acc: 0.9667 - val_loss: 0.2323 - val_acc: 0.9000\n",
      "Epoch 70/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9831\n",
      "Epoch 00070: val_loss improved from 0.23227 to 0.23215, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 127ms/sample - loss: 0.2229 - acc: 0.9833 - val_loss: 0.2321 - val_acc: 0.9000\n",
      "Epoch 71/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9661- ETA: 0s - loss: 0.2222 - acc: 0.\n",
      "Epoch 00071: val_loss improved from 0.23215 to 0.23201, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2228 - acc: 0.9667 - val_loss: 0.2320 - val_acc: 0.9000\n",
      "Epoch 72/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9831\n",
      "Epoch 00072: val_loss improved from 0.23201 to 0.23189, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2225 - acc: 0.9833 - val_loss: 0.2319 - val_acc: 0.9000\n",
      "Epoch 73/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9831\n",
      "Epoch 00073: val_loss improved from 0.23189 to 0.23186, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2225 - acc: 0.9833 - val_loss: 0.2319 - val_acc: 0.9000\n",
      "Epoch 74/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9831\n",
      "Epoch 00074: val_loss improved from 0.23186 to 0.23170, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2222 - acc: 0.9833 - val_loss: 0.2317 - val_acc: 0.9000\n",
      "Epoch 75/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9831\n",
      "Epoch 00075: val_loss improved from 0.23170 to 0.23169, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2220 - acc: 0.9833 - val_loss: 0.2317 - val_acc: 0.9000\n",
      "Epoch 76/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9831\n",
      "Epoch 00076: val_loss improved from 0.23169 to 0.23153, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2219 - acc: 0.9833 - val_loss: 0.2315 - val_acc: 0.9000\n",
      "Epoch 77/175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/60 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 1.0000\n",
      "Epoch 00077: val_loss improved from 0.23153 to 0.23136, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2219 - acc: 1.0000 - val_loss: 0.2314 - val_acc: 0.9000\n",
      "Epoch 78/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9831\n",
      "Epoch 00078: val_loss improved from 0.23136 to 0.23129, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2216 - acc: 0.9833 - val_loss: 0.2313 - val_acc: 0.9000\n",
      "Epoch 79/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 1.0000\n",
      "Epoch 00079: val_loss improved from 0.23129 to 0.23113, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 135ms/sample - loss: 0.2216 - acc: 1.0000 - val_loss: 0.2311 - val_acc: 0.9000\n",
      "Epoch 80/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 1.0000\n",
      "Epoch 00080: val_loss improved from 0.23113 to 0.23097, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 125ms/sample - loss: 0.2214 - acc: 1.0000 - val_loss: 0.2310 - val_acc: 0.9000\n",
      "Epoch 81/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 1.0000\n",
      "Epoch 00081: val_loss improved from 0.23097 to 0.23093, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2211 - acc: 1.0000 - val_loss: 0.2309 - val_acc: 0.9000\n",
      "Epoch 82/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 1.0000\n",
      "Epoch 00082: val_loss improved from 0.23093 to 0.23078, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2211 - acc: 1.0000 - val_loss: 0.2308 - val_acc: 0.9000\n",
      "Epoch 83/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 1.0000\n",
      "Epoch 00083: val_loss improved from 0.23078 to 0.23069, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2208 - acc: 1.0000 - val_loss: 0.2307 - val_acc: 0.9000\n",
      "Epoch 84/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 1.0000- ETA: 2s - loss: 0.2223 - acc: - ETA\n",
      "Epoch 00084: val_loss improved from 0.23069 to 0.23065, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2207 - acc: 1.0000 - val_loss: 0.2307 - val_acc: 0.9000\n",
      "Epoch 85/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 1.0000\n",
      "Epoch 00085: val_loss improved from 0.23065 to 0.23048, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2205 - acc: 1.0000 - val_loss: 0.2305 - val_acc: 0.9000\n",
      "Epoch 86/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 1.0000\n",
      "Epoch 00086: val_loss improved from 0.23048 to 0.23032, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2203 - acc: 1.0000 - val_loss: 0.2303 - val_acc: 0.9000\n",
      "Epoch 87/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 1.0000\n",
      "Epoch 00087: val_loss improved from 0.23032 to 0.23028, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2202 - acc: 1.0000 - val_loss: 0.2303 - val_acc: 0.9000\n",
      "Epoch 88/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 1.0000\n",
      "Epoch 00088: val_loss improved from 0.23028 to 0.23012, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 126ms/sample - loss: 0.2201 - acc: 1.0000 - val_loss: 0.2301 - val_acc: 0.9000\n",
      "Epoch 89/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 1.0000\n",
      "Epoch 00089: val_loss improved from 0.23012 to 0.23005, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2199 - acc: 1.0000 - val_loss: 0.2301 - val_acc: 0.9000\n",
      "Epoch 90/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 1.0000\n",
      "Epoch 00090: val_loss improved from 0.23005 to 0.22987, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2198 - acc: 1.0000 - val_loss: 0.2299 - val_acc: 0.9000\n",
      "Epoch 91/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 1.0000\n",
      "Epoch 00091: val_loss improved from 0.22987 to 0.22974, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2195 - acc: 1.0000 - val_loss: 0.2297 - val_acc: 0.9000\n",
      "Epoch 92/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 1.0000\n",
      "Epoch 00092: val_loss improved from 0.22974 to 0.22956, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2193 - acc: 1.0000 - val_loss: 0.2296 - val_acc: 0.9000\n",
      "Epoch 93/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 1.0000\n",
      "Epoch 00093: val_loss improved from 0.22956 to 0.22952, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2192 - acc: 1.0000 - val_loss: 0.2295 - val_acc: 0.9000\n",
      "Epoch 94/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 1.0000\n",
      "Epoch 00094: val_loss improved from 0.22952 to 0.22948, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2191 - acc: 1.0000 - val_loss: 0.2295 - val_acc: 0.9000\n",
      "Epoch 95/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 1.0000\n",
      "Epoch 00095: val_loss improved from 0.22948 to 0.22932, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2189 - acc: 1.0000 - val_loss: 0.2293 - val_acc: 0.9000\n",
      "Epoch 96/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 1.0000\n",
      "Epoch 00096: val_loss improved from 0.22932 to 0.22925, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2187 - acc: 1.0000 - val_loss: 0.2292 - val_acc: 0.9000\n",
      "Epoch 97/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 1.0000\n",
      "Epoch 00097: val_loss improved from 0.22925 to 0.22912, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2186 - acc: 1.0000 - val_loss: 0.2291 - val_acc: 0.9000\n",
      "Epoch 98/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 1.0000\n",
      "Epoch 00098: val_loss improved from 0.22912 to 0.22898, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.2183 - acc: 1.0000 - val_loss: 0.2290 - val_acc: 0.9000\n",
      "Epoch 99/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 1.0000\n",
      "Epoch 00099: val_loss improved from 0.22898 to 0.22888, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2182 - acc: 1.0000 - val_loss: 0.2289 - val_acc: 0.9000\n",
      "Epoch 100/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 1.0000- ETA: 1s - loss\n",
      "Epoch 00100: val_loss improved from 0.22888 to 0.22888, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 129ms/sample - loss: 0.2181 - acc: 1.0000 - val_loss: 0.2289 - val_acc: 0.9000\n",
      "Epoch 101/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 1.0000\n",
      "Epoch 00101: val_loss improved from 0.22888 to 0.22872, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 127ms/sample - loss: 0.2178 - acc: 1.0000 - val_loss: 0.2287 - val_acc: 0.9000\n",
      "Epoch 102/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 1.0000\n",
      "Epoch 00102: val_loss improved from 0.22872 to 0.22849, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2177 - acc: 1.0000 - val_loss: 0.2285 - val_acc: 0.9000\n",
      "Epoch 103/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.22849\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2176 - acc: 1.0000 - val_loss: 0.2285 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 1.0000\n",
      "Epoch 00104: val_loss improved from 0.22849 to 0.22833, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2174 - acc: 1.0000 - val_loss: 0.2283 - val_acc: 0.9000\n",
      "Epoch 105/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 1.0000\n",
      "Epoch 00105: val_loss improved from 0.22833 to 0.22814, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2173 - acc: 1.0000 - val_loss: 0.2281 - val_acc: 0.9000\n",
      "Epoch 106/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 1.0000\n",
      "Epoch 00106: val_loss improved from 0.22814 to 0.22802, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2171 - acc: 1.0000 - val_loss: 0.2280 - val_acc: 0.9000\n",
      "Epoch 107/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 1.0000\n",
      "Epoch 00107: val_loss improved from 0.22802 to 0.22793, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2169 - acc: 1.0000 - val_loss: 0.2279 - val_acc: 0.9000\n",
      "Epoch 108/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 1.0000\n",
      "Epoch 00108: val_loss improved from 0.22793 to 0.22784, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2167 - acc: 1.0000 - val_loss: 0.2278 - val_acc: 0.9000\n",
      "Epoch 109/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 1.0000\n",
      "Epoch 00109: val_loss improved from 0.22784 to 0.22764, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2167 - acc: 1.0000 - val_loss: 0.2276 - val_acc: 0.9000\n",
      "Epoch 110/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 1.000 - ETA: 0s - loss: 0.2172 - acc: 1.0000\n",
      "Epoch 00110: val_loss improved from 0.22764 to 0.22755, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 125ms/sample - loss: 0.2165 - acc: 1.0000 - val_loss: 0.2275 - val_acc: 0.9000\n",
      "Epoch 111/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 1.0000\n",
      "Epoch 00111: val_loss improved from 0.22755 to 0.22740, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 9s 143ms/sample - loss: 0.2164 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9000\n",
      "Epoch 112/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 1.0000\n",
      "Epoch 00112: val_loss improved from 0.22740 to 0.22739, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 128ms/sample - loss: 0.2161 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9000\n",
      "Epoch 113/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 1.0000\n",
      "Epoch 00113: val_loss improved from 0.22739 to 0.22738, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2159 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9000\n",
      "Epoch 114/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 1.0000\n",
      "Epoch 00114: val_loss improved from 0.22738 to 0.22716, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2157 - acc: 1.0000 - val_loss: 0.2272 - val_acc: 0.9000\n",
      "Epoch 115/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 1.0000\n",
      "Epoch 00115: val_loss improved from 0.22716 to 0.22698, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2156 - acc: 1.0000 - val_loss: 0.2270 - val_acc: 0.9000\n",
      "Epoch 116/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 1.0000\n",
      "Epoch 00116: val_loss improved from 0.22698 to 0.22683, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2154 - acc: 1.0000 - val_loss: 0.2268 - val_acc: 0.9000\n",
      "Epoch 117/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 1.0000\n",
      "Epoch 00117: val_loss improved from 0.22683 to 0.22671, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2153 - acc: 1.0000 - val_loss: 0.2267 - val_acc: 0.9000\n",
      "Epoch 118/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 1.0000\n",
      "Epoch 00118: val_loss improved from 0.22671 to 0.22664, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2151 - acc: 1.0000 - val_loss: 0.2266 - val_acc: 0.9000\n",
      "Epoch 119/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 1.0000\n",
      "Epoch 00119: val_loss improved from 0.22664 to 0.22652, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2149 - acc: 1.0000 - val_loss: 0.2265 - val_acc: 0.9000\n",
      "Epoch 120/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 1.0000\n",
      "Epoch 00120: val_loss improved from 0.22652 to 0.22641, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2147 - acc: 1.0000 - val_loss: 0.2264 - val_acc: 0.9000\n",
      "Epoch 121/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 1.0000\n",
      "Epoch 00121: val_loss improved from 0.22641 to 0.22628, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2145 - acc: 1.0000 - val_loss: 0.2263 - val_acc: 0.9000\n",
      "Epoch 122/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 1.0000\n",
      "Epoch 00122: val_loss improved from 0.22628 to 0.22603, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2146 - acc: 1.0000 - val_loss: 0.2260 - val_acc: 0.9000\n",
      "Epoch 123/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.22603\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2144 - acc: 1.0000 - val_loss: 0.2262 - val_acc: 0.9000\n",
      "Epoch 124/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 1.0000\n",
      "Epoch 00124: val_loss improved from 0.22603 to 0.22591, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.2141 - acc: 1.0000 - val_loss: 0.2259 - val_acc: 0.9000\n",
      "Epoch 125/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 1.0000\n",
      "Epoch 00125: val_loss improved from 0.22591 to 0.22576, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2140 - acc: 1.0000 - val_loss: 0.2258 - val_acc: 0.9000\n",
      "Epoch 126/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 1.0000\n",
      "Epoch 00126: val_loss improved from 0.22576 to 0.22572, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2138 - acc: 1.0000 - val_loss: 0.2257 - val_acc: 0.9000\n",
      "Epoch 127/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 1.0000\n",
      "Epoch 00127: val_loss improved from 0.22572 to 0.22548, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2136 - acc: 1.0000 - val_loss: 0.2255 - val_acc: 0.9000\n",
      "Epoch 128/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.22548\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.2134 - acc: 1.0000 - val_loss: 0.2255 - val_acc: 0.9000\n",
      "Epoch 129/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 1.0000\n",
      "Epoch 00129: val_loss improved from 0.22548 to 0.22543, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2132 - acc: 1.0000 - val_loss: 0.2254 - val_acc: 0.9000\n",
      "Epoch 130/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 1.0000\n",
      "Epoch 00130: val_loss improved from 0.22543 to 0.22519, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2132 - acc: 1.0000 - val_loss: 0.2252 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 1.0000\n",
      "Epoch 00131: val_loss improved from 0.22519 to 0.22508, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2128 - acc: 1.0000 - val_loss: 0.2251 - val_acc: 0.9000\n",
      "Epoch 132/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 1.0000\n",
      "Epoch 00132: val_loss improved from 0.22508 to 0.22491, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2128 - acc: 1.0000 - val_loss: 0.2249 - val_acc: 0.9000\n",
      "Epoch 133/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 1.0000\n",
      "Epoch 00133: val_loss improved from 0.22491 to 0.22481, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2126 - acc: 1.0000 - val_loss: 0.2248 - val_acc: 0.9000\n",
      "Epoch 134/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 1.0000\n",
      "Epoch 00134: val_loss improved from 0.22481 to 0.22470, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2123 - acc: 1.0000 - val_loss: 0.2247 - val_acc: 0.9000\n",
      "Epoch 135/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 1.0000\n",
      "Epoch 00135: val_loss improved from 0.22470 to 0.22462, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2122 - acc: 1.0000 - val_loss: 0.2246 - val_acc: 0.9000\n",
      "Epoch 136/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 1.0000\n",
      "Epoch 00136: val_loss improved from 0.22462 to 0.22452, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2121 - acc: 1.0000 - val_loss: 0.2245 - val_acc: 0.9000\n",
      "Epoch 137/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 1.0000\n",
      "Epoch 00137: val_loss improved from 0.22452 to 0.22437, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2119 - acc: 1.0000 - val_loss: 0.2244 - val_acc: 0.9000\n",
      "Epoch 138/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 1.0000\n",
      "Epoch 00138: val_loss improved from 0.22437 to 0.22416, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2117 - acc: 1.0000 - val_loss: 0.2242 - val_acc: 0.9000\n",
      "Epoch 139/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 1.0000\n",
      "Epoch 00139: val_loss improved from 0.22416 to 0.22402, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2116 - acc: 1.0000 - val_loss: 0.2240 - val_acc: 0.9000\n",
      "Epoch 140/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 1.0000\n",
      "Epoch 00140: val_loss did not improve from 0.22402\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2113 - acc: 1.0000 - val_loss: 0.2240 - val_acc: 0.9000\n",
      "Epoch 141/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 1.0000\n",
      "Epoch 00141: val_loss improved from 0.22402 to 0.22384, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.2112 - acc: 1.0000 - val_loss: 0.2238 - val_acc: 0.9000\n",
      "Epoch 142/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 1.0000\n",
      "Epoch 00142: val_loss improved from 0.22384 to 0.22370, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2111 - acc: 1.0000 - val_loss: 0.2237 - val_acc: 0.9000\n",
      "Epoch 143/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 1.0000\n",
      "Epoch 00143: val_loss improved from 0.22370 to 0.22361, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2109 - acc: 1.0000 - val_loss: 0.2236 - val_acc: 0.9000\n",
      "Epoch 144/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 1.0000\n",
      "Epoch 00144: val_loss improved from 0.22361 to 0.22346, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2107 - acc: 1.0000 - val_loss: 0.2235 - val_acc: 0.9000\n",
      "Epoch 145/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 1.0000\n",
      "Epoch 00145: val_loss improved from 0.22346 to 0.22336, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2106 - acc: 1.0000 - val_loss: 0.2234 - val_acc: 0.9000\n",
      "Epoch 146/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 1.0000\n",
      "Epoch 00146: val_loss improved from 0.22336 to 0.22336, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2103 - acc: 1.0000 - val_loss: 0.2234 - val_acc: 0.9000\n",
      "Epoch 147/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 1.0000\n",
      "Epoch 00147: val_loss improved from 0.22336 to 0.22324, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2102 - acc: 1.0000 - val_loss: 0.2232 - val_acc: 0.9000\n",
      "Epoch 148/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 1.0000\n",
      "Epoch 00148: val_loss improved from 0.22324 to 0.22298, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2101 - acc: 1.0000 - val_loss: 0.2230 - val_acc: 0.9000\n",
      "Epoch 149/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 1.0000\n",
      "Epoch 00149: val_loss improved from 0.22298 to 0.22293, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2099 - acc: 1.0000 - val_loss: 0.2229 - val_acc: 0.9000\n",
      "Epoch 150/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 1.0000\n",
      "Epoch 00150: val_loss improved from 0.22293 to 0.22285, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2096 - acc: 1.0000 - val_loss: 0.2228 - val_acc: 0.9000\n",
      "Epoch 151/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 1.0000\n",
      "Epoch 00151: val_loss improved from 0.22285 to 0.22269, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2095 - acc: 1.0000 - val_loss: 0.2227 - val_acc: 0.9000\n",
      "Epoch 152/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 1.0000\n",
      "Epoch 00152: val_loss improved from 0.22269 to 0.22240, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2095 - acc: 1.0000 - val_loss: 0.2224 - val_acc: 0.9000\n",
      "Epoch 153/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.22240\n",
      "60/60 [==============================] - 7s 123ms/sample - loss: 0.2094 - acc: 1.0000 - val_loss: 0.2226 - val_acc: 0.9000\n",
      "Epoch 154/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 1.0000\n",
      "Epoch 00154: val_loss improved from 0.22240 to 0.22217, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.2091 - acc: 1.0000 - val_loss: 0.2222 - val_acc: 0.9000\n",
      "Epoch 155/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 1.0000\n",
      "Epoch 00155: val_loss improved from 0.22217 to 0.22211, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2089 - acc: 1.0000 - val_loss: 0.2221 - val_acc: 0.9000\n",
      "Epoch 156/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 1.0000\n",
      "Epoch 00156: val_loss improved from 0.22211 to 0.22191, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2086 - acc: 1.0000 - val_loss: 0.2219 - val_acc: 0.9000\n",
      "Epoch 157/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 1.0000\n",
      "Epoch 00157: val_loss improved from 0.22191 to 0.22181, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2085 - acc: 1.0000 - val_loss: 0.2218 - val_acc: 0.9000\n",
      "Epoch 158/175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/60 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.22181\n",
      "60/60 [==============================] - 7s 120ms/sample - loss: 0.2083 - acc: 1.0000 - val_loss: 0.2218 - val_acc: 0.9000\n",
      "Epoch 159/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 1.0000\n",
      "Epoch 00159: val_loss improved from 0.22181 to 0.22171, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 117ms/sample - loss: 0.2081 - acc: 1.0000 - val_loss: 0.2217 - val_acc: 0.9000\n",
      "Epoch 160/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 1.0000\n",
      "Epoch 00160: val_loss improved from 0.22171 to 0.22151, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2080 - acc: 1.0000 - val_loss: 0.2215 - val_acc: 0.9000\n",
      "Epoch 161/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 1.0000\n",
      "Epoch 00161: val_loss improved from 0.22151 to 0.22138, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2079 - acc: 1.0000 - val_loss: 0.2214 - val_acc: 0.9000\n",
      "Epoch 162/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 1.0000\n",
      "Epoch 00162: val_loss improved from 0.22138 to 0.22138, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2077 - acc: 1.0000 - val_loss: 0.2214 - val_acc: 0.9000\n",
      "Epoch 163/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 1.0000\n",
      "Epoch 00163: val_loss improved from 0.22138 to 0.22109, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2074 - acc: 1.0000 - val_loss: 0.2211 - val_acc: 0.9000\n",
      "Epoch 164/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 1.0000\n",
      "Epoch 00164: val_loss did not improve from 0.22109\n",
      "60/60 [==============================] - 7s 118ms/sample - loss: 0.2073 - acc: 1.0000 - val_loss: 0.2211 - val_acc: 0.9000\n",
      "Epoch 165/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 1.0000\n",
      "Epoch 00165: val_loss improved from 0.22109 to 0.22091, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2072 - acc: 1.0000 - val_loss: 0.2209 - val_acc: 0.9000\n",
      "Epoch 166/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 1.0000\n",
      "Epoch 00166: val_loss improved from 0.22091 to 0.22081, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2071 - acc: 1.0000 - val_loss: 0.2208 - val_acc: 0.9000\n",
      "Epoch 167/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 1.0000- ETA: 0s - loss: 0.2069 - ac\n",
      "Epoch 00167: val_loss improved from 0.22081 to 0.22062, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 127ms/sample - loss: 0.2070 - acc: 1.0000 - val_loss: 0.2206 - val_acc: 0.9000\n",
      "Epoch 168/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 1.0000\n",
      "Epoch 00168: val_loss improved from 0.22062 to 0.22049, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2066 - acc: 1.0000 - val_loss: 0.2205 - val_acc: 0.9000\n",
      "Epoch 169/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 1.0000\n",
      "Epoch 00169: val_loss improved from 0.22049 to 0.22038, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 8s 125ms/sample - loss: 0.2065 - acc: 1.0000 - val_loss: 0.2204 - val_acc: 0.9000\n",
      "Epoch 170/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 1.0000\n",
      "Epoch 00170: val_loss improved from 0.22038 to 0.22038, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2063 - acc: 1.0000 - val_loss: 0.2204 - val_acc: 0.9000\n",
      "Epoch 171/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 1.0000\n",
      "Epoch 00171: val_loss improved from 0.22038 to 0.22011, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 124ms/sample - loss: 0.2061 - acc: 1.0000 - val_loss: 0.2201 - val_acc: 0.9000\n",
      "Epoch 172/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 1.0000\n",
      "Epoch 00172: val_loss improved from 0.22011 to 0.22005, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 121ms/sample - loss: 0.2059 - acc: 1.0000 - val_loss: 0.2201 - val_acc: 0.9000\n",
      "Epoch 173/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 1.0000\n",
      "Epoch 00173: val_loss improved from 0.22005 to 0.21985, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2058 - acc: 1.0000 - val_loss: 0.2199 - val_acc: 0.9000\n",
      "Epoch 174/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 1.0000\n",
      "Epoch 00174: val_loss improved from 0.21985 to 0.21979, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 119ms/sample - loss: 0.2056 - acc: 1.0000 - val_loss: 0.2198 - val_acc: 0.9000\n",
      "Epoch 175/175\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 1.0000\n",
      "Epoch 00175: val_loss improved from 0.21979 to 0.21967, saving model to siamese_network.h5\n",
      "60/60 [==============================] - 7s 122ms/sample - loss: 0.2055 - acc: 1.0000 - val_loss: 0.2197 - val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" train the model \"\"\"\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "siamese.compile(loss=utils.loss(1), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# siamese.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "siamese.summary()\n",
    "history = siamese.fit([x_train_1, x_train_2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val_1, x_val_2], labels_val),\n",
    "    batch_size=1,\n",
    "    epochs=175,   # 175 for contrastive 100 for cross ent\n",
    "    callbacks = [checkpointer, early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5219b8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy\n",
    "utils.plt_metric(history=history.history, metric=\"acc\", title=\"Model accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb37692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the constrastive loss\n",
    "utils.plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ebb1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296/1296 [==============================] - 52s 40ms/sample - loss: 0.2094 - acc: 0.9691\n",
      "test loss, test acc: [0.2094487930521553, 0.9691358]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test the model \"\"\"\n",
    "results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be00d433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46319255, 0.4400196 , 0.53185886, ..., 0.37433347, 0.53078866,\n",
       "       0.36303332], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = siamese.predict([x_test_1, x_test_2]).squeeze()\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6adf7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44620532"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4ec4ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False,  True, False])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_pred = Y_pred > .5\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f3303c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = labels_test\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c08c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on test data\n",
      "Accuracy: 0.9691358024691358\n",
      "Precision: 0.9705882352941178\n",
      "Recall: 0.9691358024691358\n",
      "ROC AUC: 0.9691358024691358\n",
      "F1: 0.9691119691119691\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluate on test data\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1:\", f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3661b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.9969135802469136\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)    \n",
    "# cm_display = ConfusionMatrixDisplay(cm, labels_test).plot()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10419799",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ff4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
